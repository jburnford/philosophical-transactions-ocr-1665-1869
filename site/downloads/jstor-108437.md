# General Methods in Analysis for the Resolution of Linear Equations in Finite Differences and Linear Differential Equations

**Author(s):** Charles James Hargreave  
**Year:** 1850  
**Journal:** Philosophical Transactions of the Royal Society of London  
**Volume:** 140  
**Pages:** 27 pages  
**Identifier:** jstor-108437  
**JSTOR URL:** <https://www.jstor.org/stable/108437>  

---

XIII. General Methods in Analysis for the resolution of Linear Equations in Finite Differences and Linear Differential Equations. By Charles James Hargreave, Esq., L.L.B., F.R.S., Professor of Jurisprudence in University College, London.

Received March 15,â€”Read March 29, 1849.

Preliminary Remarks.

1. The investigations presented in this paper consist of two parts; the first offers a solution, in a certain qualified sense, of the general linear equation in finite differences; and the second will be found to give an almost complete analysis of the resolution in series of the general linear differential equation with rational factors. The second part is deduced directly from the results of the first, although the subjects of which they respectively treat appear to be wholly independent of each other.

With the exception of a few cases capable of solution by partial and artificial methods, there does not at present exist any mode of solving linear equations in finite differences of an order higher than the first; and with reference to such equations of the first order, we are obliged to be content with those insufficient forms of functions which are intelligible only when the independent variable is an integer, and which may be obtained directly from the equation itself by merely giving to the independent variable its successive integer values. It is in this insufficient and qualified sense that the solutions here given are to be taken; and the first part of the following investigations may be considered as an extension of this form of solution from the general equation of the first order to the general equation of the nth order.

Linear Equations in Finite Differences.

2. A complete analytical theory of the general equation of the nth order,

\[ u_x = P_x u_{x-1} + Q_x u_{x-2} + R_x u_{x-3} + S_x u_{x-4} + \ldots + W_x u_{x-n+1} + Z_x u_{x-n} + G_x \quad \ldots \quad (1.) \]

would involve its resolution into a series of equations of the first order of the form

\[ u_x - P'_x u_{x-1} = G'_x, \]
\[ u_x - P''_x u_{x-1} = G''_x, \]
\[ \ldots \]
\[ u_x - P^{(n)}_x u_{x-1} = G^{(n)}_x; \]
from which the complete solution would result, so far as known methods extend, in the form

\[ u_x = P_1'P_2'\ldots P_x'\Sigma \left( \frac{G_x'}{P_1'P_2'\ldots P_{x+1}'} \right) + \ldots + P^{(n)}_1P^{(n)}_2\ldots P^{(n)}_x\Sigma \left( \frac{G^{(n)}_x}{P^{(n)}_1P^{(n)}_2\ldots P^{(n)}_{x+1}} \right), \]

or in an expanded form,

\[ u_x = G'_x + P'_xG'_{x-1} + P'_{x-1}P'_xG'_{x-2} + P'_{x-2}P'_{x-1}P'_xG'_{x-3} + \ldots + c'P'_mP'_m\ldots P'_x \]
\[ + G''_x + P''_xG''_{x-1} + P''_{x-1}P''_xG''_{x-2} + P''_{x-2}P''_{x-1}P''_xG''_{x-3} + \ldots + c''P''_mP''_m\ldots P''_x \]
\[ + \ldots \]
\[ + G^{(n)}_x + P^{(n)}_xG^{(n)}_{x-1} + P^{(n)}_{x-1}P^{(n)}_xG^{(n)}_{x-2} + P^{(n)}_{x-2}P^{(n)}_{x-1}P^{(n)}_xG^{(n)}_{x-3} + \ldots + c^{(n)}P^{(n)}_mP^{(n)}_m\ldots P^{(n)}_x. \]

The last column contains, as is well known, the general solution of the original equation deprived of the term \( G_x \); and the remaining columns contain the particular solution of the original equation.

3. The investigation the results of which are given in this part, although it actually succeeds in solving the original equation, will be found to contribute little or nothing to the analytical theory as above explained; and this arises from the circumstance, that the particular solution, instead of being produced in the separated form above written, is produced in an aggregated form,

\[ G_x + \Lambda G_{x-1} + BG_{x-2} + \ldots; \]

and the complementary part of the general solution, instead of being produced as above, appears in the form of a sum of the complementary functions above written, the constants \( c' \), \( c'' \), &c. being the same in all. But as we shall thus obtain a solution with an arbitrary constant, capable of solving the original equation deprived of \( G_x \), we have it in our power to solve the equation completely by reducing the order of the equation in successive steps.

If we take the equation of the first order,

\[ u_x = P_xu_{x-1} + G_x, \]

and arrive at its solution, not properly by any analytical method, but by giving to \( x \) the successive values

\[ x, x-1, x-2, \ldots, m+1, m, m-1, \]

and eliminating all values of \( u_x \) between the first and the last, we have

\[ u_x = P_xu_{x-1} + G_x \]
\[ = P_x(P_{x-1}u_{x-2} + G_{x-1}) + G_x \text{ or } P_xP_{x-1}u_{x-2} + P_xG_{x-1} + G_x \]
\[ = P_xP_{x-1}(P_{x-2}u_{x-3} + G_{x-2}) + P_xG_{x-1} + G_x \text{ or } P_xP_{x-1}P_{x-2}u_{x-3} + P_xP_{x-1}G_{x-2} + P_xG_{x-1} + G_x \]
\[ = \ldots \]
\[ = (P_xP_{x-1}\ldots P_m)u_{m-1} + (P_x\ldots P_{m+1})G_m + (P_x\ldots P_{m+2})G_{m+1} + \ldots + P_xP_{x-1}G_{x-2} + P_xG_{x-1} + G_x \]
\[
= (P_x \ldots P_m) \left\{ u_{m-1} + \frac{G_m}{P_m} + \frac{G_{m+1}}{P_m P_{m+1}} + \ldots + \frac{G_{x-2}}{P_m \ldots P_{x-2}} + \frac{G_{x-1}}{P_m \ldots P_{x-1}} + \frac{G_x}{P_m \ldots P_x} \right\}
\]

\[= P_x \ldots P_m \left( \sum \frac{G_{x+1}}{P_m \ldots P_{x+1}} + c \right); \]

which is the solution ordinarily given, though it is arrived at by a process somewhat less coarse than the above.

By applying the same process of successive elimination to the general linear equation (1.), and carefully observing the law by which the entrance of the factors \(P_x, Q_x, R_x, \text{ &c.}\) is governed, it will be found that an exactly similar solution may be found in the form

\[u_x = \varepsilon^v \left\{ P_x \ldots P_m \left( \sum \frac{G_{x+1}}{P_m \ldots P_{x+1}} \right) + c \right\},\]

or

\[u_x = G_x + P_x G_{x-1} + \varepsilon^v(P_x P_{x-1}) G_{x-2} + \varepsilon^v(P_x \ldots P_{x-2}) G_{x-3} + \ldots + \varepsilon^v(P_x \ldots P_{m+1}) G_m + c \varepsilon^v(P_x \ldots P_m);\]

where \(v\) denotes the sum of a set of distributive operations \(V_1, V_2, V_3, \ldots V_{n-1}\) not of a strictly algebraical character, which are capable of being performed only upon factorial expressions containing consecutive values of \(P_x\) and which have the following significations. \(V_1\) denotes one operation of this character, signifying that the factor \(P_{m-1} P_m\) is changed into \(Q_m\) as often as it occurs, any term in which it does not occur disappearing, and the sum of the terms thus obtained being the result of the operation; so that, for example,

\[V_1(P_{x-1} P_x) = Q_x, \quad V_1(P_{x-1} P_x) = 0,\]

\[V_1(P_{x-2} P_{x-1} P_x) = Q_{x-1} P_x + P_{x-2} Q_x, \quad V_1(P_{x-2} P_{x-1} P_x) = 0,\]

\[V_1(P_{x-3} P_{x-2} P_{x-1} P_x) = Q_{x-2} P_{x-1} P_x + P_{x-3} Q_{x-1} P_x + P_{x-3} P_{x-2} Q_x, \quad V_1(P_{x-3} P_{x-2} P_{x-1} P_x) = 2 Q_{x-2} Q_x,\]

&c. &c.

Again, \(V_2\) denotes another operation of a similar character, signifying that the factor \(P_{m-2} P_{m-1} P_m\) is changed into \(R_m\) as often as it occurs, the result of the operation being as before the sum of the terms; so that, for example, we have

\[V_2(P_{x-3} P_{x-2} P_{x-1} P_x) = R_{x-1} P_x + P_{x-3} R_x,\]

\[V_2(P_{x-4} P_{x-3} P_{x-2} P_{x-1} P_x) = R_{x-2} P_{x-1} P_x + P_{x-4} R_{x-1} P_x + P_{x-4} P_{x-3} R_x,\]

\[V_2(P_{x-5} \ldots P_x) = R_{x-3} P_{x-2} P_{x-1} P_x + P_{x-5} R_{x-2} P_{x-1} P_x + P_{x-5} P_{x-4} R_{x-1} P_x + P_{x-5} P_{x-4} P_{x-3} R_x,\]

\[V_2(P_{x-5} \ldots P_x) = 2 R_{x-3} R_x, \text{ &c. &c.;}\]

\(V_3\) denotes the change in a similar manner of \(P_{m-3} P_{m-2} P_{m-1} P_m\) into \(S_m;\)

&c. &c.

\(V_{n-2}\) denotes the change in a similar manner of \(P_{m-n+2} \ldots P_m\) into \(W_m;\)

and

\(V_{n-1}\) denotes the change in a similar manner of \(P_{m-n+1} \ldots P_m\) into \(Z_m.\)
It might at first sight be supposed that this process, if successful at all, would give the complete complementary solution in the form

\[ u_x = c_1 \varepsilon^x(P_1 \ldots P_x) + c_2 \varepsilon^x(P_2 \ldots P_x) + \ldots; \]

but it will be found that in reality this introduces only one arbitrary constant.

It now remains to place these expressions under a properly algebraic form, and to verify the result; and in order to do this we must express the general term \( \varepsilon^x(P_x \ldots P_{x+1}) \) in terms of the factors of the original equation; and afterwards give to \( p \), which in the first instance is regarded as a constant, the successive values required for forming the several terms of the solution.

This may be done as follows:

Let \( \frac{Q_x}{P_{x-1} \ldots P_x} = q_x, \frac{R_x}{P_{x-2} \ldots P_x} = r_x, \frac{S_x}{P_{x-3} \ldots P_x} = s_x, \ldots \frac{W_x}{P_{x-n+2} \ldots P_x} = w_x, \) and \( \frac{Z_x}{P_{x-n+1} \ldots P_x} = z_x. \)

Then it is easily seen that

\[ V_1(P_x \ldots P_{x+1}) = P_x \ldots P_{x+1}(q_{x+2} + q_{x+3} + \ldots + q_x) = P_x \ldots P_{x+1} \sum_{x+1}^{x} q_{x+1}, \]

\[ V_2(P_x \ldots P_{x+1}) = P_x \ldots P_{x+1}(r_{x+3} + r_{x+4} + \ldots + r_x) = P_x \ldots P_{x+1} \sum_{x+1}^{x} r_{x+1}, \]

and generally

\[ v(P_x \ldots P_{x+1}) = P_x \ldots P_{x+1}(\sum_{x+1}^{x} q_{x+1} + \sum_{x+2}^{x} r_{x+1} + \sum_{x+3}^{x} s_{x+1} + \ldots + \sum_{x+n}^{x} w_{x+1} + \sum_{x+n-1}^{x} z_{x+1}). \]

To find \( V_1^2(P_x \ldots P_{x+1}) \), it will be convenient to proceed by steps, beginning with a small number of terms.

Thus

\[ V_1(P_x \ldots P_{x-3}) = (P_x \ldots P_{x-3})(q_x + q_{x-1} + q_{x-2}), \quad \frac{1}{2} V_1^2(P_x \ldots P_{x-3}) = P_x \ldots P_{x-3}(q_x q_{x-2}) \]

\[ V_1(P_x \ldots P_{x-4}) = (P_x \ldots P_{x-4})(q_x + q_{x-1} + q_{x-2} + q_{x-3}), \quad \frac{1}{2} V_1^2(P_x \ldots P_{x-4}) = P_x \ldots P_{x-4}(q_x(q_{x-2} + q_{x-3}) + q_{x-1}q_{x-2}) \]

\[ V_1(P_x \ldots P_{x-5}) = (P_x \ldots P_{x-5})(q_x + q_{x-1} + q_{x-2} + q_{x-3} + q_{x-4}), \]

\[ \frac{1}{2} V_1^2(P_x \ldots P_{x-5}) = (P_x \ldots P_{x-5})(q_x(q_{x-2} + q_{x-3} + q_{x-4}) + q_{x-1}(q_{x-3} + q_{x-4}) + q_{x-2}q_{x-4}) \text{ &c.;} \]

and generally, \( V_1(P_x \ldots P_{x+1}) \) being \( P_x \ldots P_{x+1}(\sum_{x+1}^{x} q_{x+1}) \), we have

\[ \frac{1}{2} V_1^2(P_x \ldots P_{x+1}) = P_x \ldots P_{x+1}(q_x \sum_{x+1}^{x} q_{x+1} + q_{x-1} \sum_{x+1}^{x} q_{x+1} + \ldots + q_{x+4} \sum_{x+1}^{x} q_{x+1}) \]

\[ = P_x \ldots P_{x+1} \sum_{x+1}^{x} (q_{x+1} \sum_{x+1}^{x} q_{x+1}). \]

Similarly, it will be seen that

\[ \frac{1}{2} V_1^3(P_x \ldots P_{x+1}) = P_x \ldots P_{x+1} \sum_{x+1}^{x} (q_{x+1} \sum_{x+1}^{x} q_{x+1}), \] and so on.

In like manner we shall find

\[ \frac{1}{2} V_2^2(P_x \ldots P_{x+1}) = P_x \ldots P_{x+1} \sum_{x+1}^{x} (r_{x+1} \sum_{x+1}^{x} r_{x+1}). \]
\[
\frac{1}{2} V^3_2(P_x \ldots P_{p+1}) = P_x \ldots P_{p+1} \sum_{x+1}^{x+2}(r_{x+1} \sum_{x+2}^{x+3}(r_{x+1} \sum_{x+3}^{x+4}(r_{x+1} \ldots)))
\]
and so on,

\[
\frac{1}{2} V^2_m(P_x \ldots P_{p+1}) = (P_x \ldots P_{p+1}) \sum_{x+1}^{x+m+1}(v_{x+1} \sum_{x+2}^{x+m+2}(v_{x+1} \ldots))
\]

\[
\frac{1}{2} V^3_m(P_x \ldots P_{p+1}) = (P_x \ldots P_{p+1}) \sum_{x+1}^{x+m+2}(v_{x+1} \sum_{x+2}^{x+m+3}(v_{x+1} \ldots))
\]
and so on,

\(v_x\) being that term of the series \(q_x r_x s_x, \ldots\) which corresponds to the operation \(V_m\), as \(q_x\) corresponds to \(V_1\), \(r_x\) to \(V_2\), \&c.

\[
V_1 V_2(P_x \ldots P_{p+1}) = P_x \ldots P_{p+1} \sum_{x+1}^{x+2}(r_{x+1} \sum_{x+2}^{x+3}(q_{x+1} \ldots))
\]

\[
V_l V_m(P_x \ldots P_{p+1}) = P_x \ldots P_{p+l+m+1}(t_{x+1} \sum_{x+2}^{x+l+m+1}(t_{x+1} \ldots))
\]
\((t_x\) corresponding to \(V_l\),

\[
V^2_l V_m(P_x \ldots P_{p+1}) = P_x \ldots P_{p+l+m+2}(t_{x+1} \sum_{x+2}^{x+l+m+2}(t_{x+1} \ldots))
\]
\(&c.\ &c.\)

Finally, if \(N'_x, p + N''_x, p + N'''_x, p + \ldots\) represent a series in which

\[
N'_x, p = 1, \quad N''_x, p = \sum_{x+1}^{x+2}(q_{x+1} \sum_{x+2}^{x+3}(r_{x+1} \ldots))
\]

and generally

\[
N^{(a+1)}_x, p = \sum_{x+1}^{x+a-1}(q_{x+1} N^{(a)}_{x-1, p}) + \sum_{x+1}^{x+a-1}(r_{x+1} N^{(a)}_{x-2, p}) + \sum_{x+1}^{x+a-1}(s_{x+1} N^{(a)}_{x-3, p}) + \ldots
\]

\[
+ \sum_{x+1}^{x+(n-1)a-1}(w_{x+1} N^{(a)}_{x-n+2, p}) + \sum_{x+1}^{x+na-1}(z_{x+1} N^{(a)}_{x-n+1, p});
\]

then, if this series be called \(N_x, p\), we shall find that \(v(P_x \ldots P_{p+1}) = (P_x \ldots P_{p+1}) N_x, p\).

It may be here observed that the number of the terms of the series \(N'_x, p + N''_x, p + \ldots\) cannot exceed \(\frac{1}{2}(x-p)+1\) when \(x-p\) is even, and \(\frac{1}{2}(x-p)+1\) when \(x-p\) is odd; and since \(p\) has the successive values \(x-2, x-3, \ldots\), it is always known whether \(x-p\) be odd or even. The number of terms may be less; for if \(Q_x\) be zero, the number of terms would be the next whole number above \(\frac{1}{3}(x-p)\), \&c.

That the equation

\[
u_x = c(P_x \ldots P_{p+1}) N_x, p
\]

is a complementary solution of the original equation, or in other words, a solution of the original equation wanting the term \(G_x\), may be directly verified; for we have

\[
u_x - P_x u_{x-1} = c(P_x \ldots P_{p+1})(N_x, p - N_{x-1, p})
\]

\[
= c(P_x \ldots P_{p+1}) \Delta N_{x-1, p}.
\]

Now

\[
\Delta N'_x, p = 0,
\]

\[
\Delta N''_x, p = q_x + r_x + s_x + \ldots + z_x,
\]

\[
\Delta N'''_x, p = q_x N''_{x-2, p} + r_x N''_{x-3, p} + s_x N''_{x-4, p} + \ldots + w_x N''_{x-n+1, p} + z_x N''_{x-n, p},
\]

and generally

\[
\Delta N^{(a+1)}_x, p = q_x N^{(a)}_{x-2, p} + r_x N^{(a)}_{x-3, p} + s_x N^{(a)}_{x-4, p} + \ldots + w_x N^{(a)}_{x-n+1, p} + z_x N^{(a)}_{x-n, p},
\]

whence

\[
\Delta N_x, p = q_x N_{x-2, p} + r_x N_{x-3, p} + s_x N_{x-4, p} + \ldots + w_x N_{x-n+1, p} + z_x N_{x-n, p};
\]

and

\[
u_x - P_x u_{x-1} = Q_x u_{x-2} + R_x u_{x-3} + S_x u_{x-4} + \ldots + W_x u_{x-n+1} + Z_x u_{x-n}.
\]
In this part of the solution, I apprehend that it will not generally be necessary to have regard to the lower limits, since \( p \) may have any constant value, and that value may be taken which is most convenient in each case; and if we make \( p = -\infty \), all the lower limits will have this value.

This part of the solution would then be

\[
u_x = cP_x \ldots P_1 \{1 + A_x + B_x + C_x + \ldots\},
\]

where

\[
A_x = \Sigma q_{x+1} + \Sigma r_{x+1} + \Sigma s_{x+1} + \ldots + \Sigma w_{x+1} + \Sigma z_{x+1};
\]

and generally each term of the part within the brackets is formed from the preceding term thus: first change \( x \) into \( x-1 \), multiply by \( q_{x+1} \), and effect a summation; then change (in such preceding term) \( x \) into \( x-2 \), multiply by \( r_{x+1} \), and effect a summation; and so on until lastly we change \( x \) into \( x-n+1 \), multiply by \( z_{x+1} \), and effect a summation; and the sum of the parts thus obtained is the next term.

The verification of the particular solution is easily derived from the above; but it rests on the assumption that the algebraic value above given for \( \varepsilon'(P_x \ldots P_{p+1}) \) is correct; to which therefore particular attention is directed.

The equation

\[
u_x = c\varepsilon'(P_x \ldots P_{p+1}),
\]

considered as a solution of the original equation wanting the term \( G_x \), implies that

\[
\varepsilon''(P_x \ldots P_{p+1}) = P_x \varepsilon'(P_{x-1} \ldots P_{p+1}) + Q_x \varepsilon'(P_{x-2} \ldots P_{p+1}) + R_x \varepsilon'(P_{x-3} \ldots P_{p+1}) + \ldots + Z_x \varepsilon'(P_{x-n} \ldots P_{p+1}).
\]

Now in order to verify the particular solution,

\[
u_x = G_x + P_x G_{x-1} + \varepsilon'(P_x P_{x-1}) G_{x-2} + \varepsilon'(P_x P_{x-1} P_{x-2}) G_{x-3} + \ldots + \varepsilon'(P_x \ldots P_{x-p+1}) G_{x-p} + \ldots,
\]

it is only requisite that

\[
\varepsilon''(P_x \ldots P_{x-p+1}) = P_x \varepsilon'(P_{x-1} \ldots P_{x-p+1}) + Q_x \varepsilon'(P_{x-2} \ldots P_{x-p+1}) + R_x \varepsilon'(P_{x-3} \ldots P_{x-p+1}) + \ldots + Z_x \varepsilon'(P_{x-n} \ldots P_{x-p+1});
\]

and this is true, for it is identical in form with the equation last above given, notwithstanding the occurrence of \( x \) in the lowest value of \( P \), for this lowest value remains the same throughout the expression.

7. If \( P_x = 0 \), the expressions \( \varepsilon''(P_x \ldots P_{x-m}) \) reduce themselves to those terms which do not contain any value of \( P \). It would not be difficult to determine generally what terms these are; but probably the most convenient general method of arriving at the solution in an algebraical form would be to make \( P_x \) equal to a constant \( b \), and to make \( b \) equal to zero in the result finally obtained.

8. The above particular solution of the original equation is in such a form that the general term of the indefinite series representing the solution is given in explicit terms; but that general term may be represented in an implicit form, which perhaps is more convenient for practical use.

By attending to the formation of the successive expressions \( \varepsilon''(P_x \ldots P_m) \), it will readily be seen that the series

\[
M_0 G_x + M_1 G_{x-1} + M_2 G_{x-2} + \ldots + M_p G_{x-p} + \ldots
\]
is a solution of the original equation, if

\[ M_0 = 1, \]
\[ M_1 = P_x M_0, \]
\[ M_2 = P_{x-1} M_1 + Q_x M_0, \]
\[ M_3 = P_{x-2} M_2 + Q_{x-1} M_1 + R_x M_0, \]
\[ \vdots \]
\[ M_p = P_{x-p+1} M_{p-1} + Q_{x-p+2} M_{p-2} + R_{x-p+3} M_{p-3} + \cdots + W_{x-p+n-1} M_{p-n+1} + Z_{x-p+n} M_{p-n}; \]

which last is a general relation determining the coefficient of any term from the coefficients of the \( n \) preceding terms; or from the coefficients of all the preceding terms when the number of these preceding terms is less than \( n \). This relation is the equation of formation, and may be regarded as universal, bearing in mind that when \( p \) is less than \( n \) some of the terms of this relation vanish.

The solution of the original equation is therefore reduced to that of a similar equation without second member: and, by what has preceded, this solution is

\[ M_p = \varepsilon^n (P_{x-p+1} \ldots P_x), \]

it being understood that \( M_0 = 1 \); and this is the value of \( M_p \) before given.

But the value of \( M_p \) may be found otherwise, thus:

Make

\[ P_{x-p+1} = P'_p, \quad Q_{x-p+2} = Q'_p, \quad \text{&c. &c.} \]

Then the solution is evidently

\[ M_p = \varepsilon^n (P'_p \ldots P'_1), \]

where the operation \( v \) has the same meaning as before, except that it is applied to the accented letters.

Consequently

\[ M_p = (P'_p \ldots P'_1) \{1 + A_p + B_p + C_p + \ldots\}, \]

where

\[ A_p = \Sigma q'_{p+1} + \Sigma r'_{p+1} + \ldots + \Sigma z'_{p+1}, \]
\[ B_p = \Sigma (q'_{p+1} A_{p-1}) + \Sigma (r'_{p+1} A_{p-2}) + \ldots + \Sigma (z'_{p+1} A_{p-x+1}), \]

and generally the terms are formed as stated in section 5, using \( p \) for \( x \).

9. I shall conclude this part of the subject with a few simple examples for the purpose of illustrating the processes here given.

Ex. 1. Let the equation be

\[ u_x = a u_{x-1} + b^2 u_{x-2} + G_x. \]

That part of the solution which is independent of \( G_x \), is

\[ u_x = c \{a^x + (x-1)a^{x-2}b^2 + \frac{1}{2}(x-2)(x-3)a^{x-4}b^4 + \frac{1}{2,3}(x-3)(x-4)(x-5)a^{x-6}b^6 + \ldots\}; \]
and since in the present case $P_x$ and $Q_x$ are constants, we have

$$M_p = a^p + (p-1)a^{p-2}b^2 + \frac{1}{2}(p-2)(p-3)a^{p-4}b^4 + \frac{1}{2\cdot 3}(p-3)(p-4)(p-5)a^{p-6}b^6 + \ldots$$

whence the particular value of $u_x$ is a series of terms of the form

$$G_x + aG_{x-1} + (a^2 + b^2)G_{x-2} + (a^3 + 2ab^2)G_{x-3} + (a^4 + 3a^2b^2 + b^4)G_{x-4} + (a^5 + 4a^3b^2 + 3ab^4)G_{x-5} + \ldots$$

The difference in character between the solution proposed in this paper, and that which would result from a perfect analysis of the general equation, may be exemplified by the present instance.

The perfect solution of this equation is known to be

$$u_x = \frac{1}{\alpha - \beta} \left( \alpha \Sigma G_{x+2} - \beta \Sigma G_{x+2} \right),$$

where $\alpha$ and $\beta$ are the roots of $t^2 - at - b^2 = 0$, and each $\Sigma$ introduces a constant.

Now if the constants be made equal, and the expression be written at length, we shall obtain the form derived above.

The expansion of the two terms within the parenthesis gives

$$G_{x+1} + \alpha G_x + \alpha^2 G_{x-1} + \ldots + \alpha^{p+1} G_p + \ldots + c\alpha^x$$

and

$$G_{x+1} + \beta G_x + \beta^2 G_{x-1} + \ldots + \beta^{p+1} G_p + \ldots + c\beta^x;$$

and if the difference of these be divided by $\alpha - \beta$, we get

$$G_x + \frac{\alpha^2 - \beta^2}{\alpha - \beta} G_{x-1} + \ldots + \frac{\alpha^{p+1} - \beta^{p+1}}{\alpha - \beta} G_p + \ldots + c\frac{\alpha^x - \beta^x}{\alpha - \beta},$$

which is the same in effect as the result which we have obtained.

**Ex. 2.** Let the equation be

$$u_x = xu_{x-1} + au_{x-2}.$$

Then

$$u_x = c\Gamma(x+1)\left[1 + a\Sigma x \frac{1}{x(x+1)} + a^2\Sigma \left(\frac{1}{x(x+1)} \Sigma x^{-1} \frac{1}{x(x+1)}\right) + \ldots\right]$$

$$= c\Gamma(x+1)\left[1 - a \frac{1}{x} + a^2 \frac{1}{(x-1)x} - \frac{a^3}{2\cdot 3} \frac{1}{(x-2)(x-1)x} + \ldots\right]$$

$$= c\left\{\Gamma(x+1) - a\Gamma x + \frac{a^2}{2} \Gamma(x-1) - \frac{a^3}{2\cdot 3} \Gamma(x-2) + \ldots\right\};$$

which may be put under the form of the definite integral

$$u_x = c \int_0^\infty e^{-v} v^x dv.$$

If we add a second member $G_x$, the equation for determining $M_p$ is

$$M_p = (x-p+1)M_{p-1} + aM_{p-2};$$
whence

\[ M_p = x \ldots (x-p+1) \left\{ 1 + a \sum_{i=1}^{p} \frac{1}{(x-p+1)(x-p)} + a^2 \sum_{i=1}^{p-1} \frac{1}{(x-p+1)(x-p)} \right\} + \ldots \]

\[ = x \ldots (x-p+1) \left\{ 1 + a \left( \frac{1}{x-p+1} - \frac{1}{x} \right) + a^2 \left( \frac{1}{2} \left( \frac{1}{(x-p+2)(x-p+1)} - \frac{1}{(x-1)(x-2)} \right) - \frac{1}{x} \left( \frac{1}{x-p+1} - \frac{1}{x-2} \right) \right) + \ldots \right\} \]

from which the series for \( u_x \) can be expressed.

**Ex. 3.** Let the equation be

\[ u_x = au_{x-1} + xu_{x-2}. \]

Then

\[ u_x = ca^x \left\{ 1 + \frac{1}{a^2} \Sigma^x(x+1) + \frac{1}{a^4} \Sigma^x((x+1)\Sigma^{x-1}(x+1)) + \ldots \right\} \]

\[ = c \left\{ a^x + \frac{1}{2} x(x+1)a^{x-2} + \frac{1}{2.4} (x+1)x(x-1)(x-2)a^{x-4} + \frac{1}{2.4.6} (x+1)x \ldots (x-4)a^{x-6} + \ldots \right\}. \]

**Ex. 4.** Let the equation be

\[ u_x = \frac{n}{x} u_{x-1} + c^2 u_{x-2}. \]

Then

\[ u_x = \frac{n^x}{\Gamma(x+1)} \left\{ 1 + \frac{c^2}{x^2} \Sigma^x x(x+1) + \frac{c^4}{x^4} \Sigma^x (x(x+1)\Sigma^{x-1}x(x+1)) + \ldots \right\} \]

\[ = \frac{n^x}{\Gamma(x+1)} \left\{ 1 + \frac{c^2}{x^2} \frac{1}{3}(x-1)x(x+1) + \frac{c^4}{x^4} \left( \frac{1}{6} (x+1)x(x-1)(x-2)(x-3)(x-4) + \frac{3}{5} (x+1)x(x-1)(x-2)(x-3) \right) + \ldots \right\}. \]

**Ex. 5.** Let the equation be

\[ u_x = P_x u_{x-1} + xP_x P_{x-1} u_{x-2} + G_x. \]

Here

\[ q_x = x, N'_{x,p} = 1, N''_{x,p} = \frac{x(x+1)}{2} - \frac{(p+1)(p+2)}{2}, \]

\[ N'''_{x,p} = \frac{(x+1)x(x-1)(x-2)}{2.4} - \frac{(p+2)(p+1)(x+1)x}{2} + \frac{(p+4)(p+3)(p+2)(p+1)}{2.4}, \]

\[ N^{(iv)}_{x,p} = \frac{(x+1)x(x-1)(x-2)(x-3)(x-4)}{2.4.6} - \frac{(p+2)(p+1)(x+1)x(x-1)(x-2)}{2.4} \]

\[ + \frac{(p+4)(p+3)(p+2)(p+1)(x+1)x}{2.4} - \frac{(p+6)(p+5)(p+4)(p+3)(p+2)(p+1)}{2.4.6}, \]

\[ N^{(m+2)}_{x,p} = \frac{(x+1)\ldots(x-2m)}{2.4\ldots(2m+2)} - \frac{(p+2)(p+1)(x+1)\ldots(x-2m+2)}{2.4\ldots2m} + \frac{(p+4)\ldots(p+1)(x+1)\ldots(x-2m+4)}{2.4\ldots(2m-2)} \]

\[ + \ldots \pm \frac{(p+2m)\ldots(p+1)(x+1)x}{2.4\ldots2m} + \frac{(p+2m+2)\ldots(p+1)}{2.4\ldots(2m+2)}. \]
The particular solution, therefore, is

\[ u_x = G_x + P_x G_{x-1} + (1+x)P_x P_{x-1} G_{x-2} + 2xP_x P_{x-1} P_{x-2} G_{x-3} + (x^2 + x - 2)(P_x \ldots P_{x-3}) G_{x-4} + (3x^2 - 5x - 2)(P_x \ldots P_{x-4}) G_{x-5} + (x^3 - 11x + 6)(P_x \ldots P_{x-5}) G_{x-6} + \ldots; \]

and the part of the general solution found by this method is

\[ u_x = c(P_x \ldots P_0) \left( 1 + \frac{(x+1)x}{2} + \frac{(x+1)\ldots(x-2)}{2\cdot4} + \frac{(x+1)\ldots(x-4)}{2\cdot4\cdot6} + \ldots \right). \]

**Ex. 6.** Let the equation be

\[ u_x + a_1 P_x u_{x-1} + a_2 P_x P_{x-1} u_{x-2} + \ldots + a_n (P_x \ldots P_{x-n+1}) u_{x-n} = G_x; \]

then the equation for determining \( M_p \) is

\[ M_p + a_1 P_{x-p+1} M_p + a_2 P_{x-p+1} P_{x-p+2} M_{p-2} + \ldots + a_n (P_{x-p+1} \ldots P_{x-p+n}) M_{p-n} = 0; \]

or making

\[ P_{x-p+1} = P'_p, \]

\[ M_p + a_1 P'_p M_{p-1} + a_2 P'_p P'_{p-1} M_{p-2} + \ldots + a_n (P'_p \ldots P'_{p-n+1}) M_{p-n} = 0; \]

the solution of which is evidently the solution of

\[ M_p + a_1 M_{p-1} + a_2 M_{p-2} + \ldots + a_n M_{p-n} = 0, \]

multiplied by \((P'_p \ldots P'_i)\).

Let

\[ (t^n + a_1 t^{n-1} + a_2 t^{n-2} + \ldots + a_n)^{-1} = \frac{A}{t-\alpha} + \frac{B}{t-\beta} + \ldots, \]

then

\[ M_p = (P'_p \ldots P'_i) \{ c_1 A \omega^{p+n} + c_2 B \beta^{p+n} + \ldots \}; \]

and taking the parts affected by each constant separately, it will be seen that the original equation reduces itself to a set of equations of the first order,

\[ v_x - \alpha P_x v_{x-1} = \alpha^{n-1} AG_x, \]
\[ v_x - \beta P_x v_{x-1} = \beta^{n-1} BG_x, \]
\[ \vdots \]
\[ \vdots \]

so that its complete solution can be adequately represented.

**Solution of Differential Equations in Series.**

10. I now proceed to point out a method by which the processes above indicated may be made to give solutions of certain general forms of linear differential equations.

In a paper on Linear Differential Equations presented by me to the Royal Society, and which the Society has done me the honour to publish in the Philosophical Transactions (Part I. for 1848, p. 31), I have enunciated, and so far as is material to the present purpose, demonstrated the following theorem:

That if, in a linear differential expression \( \phi(x, D)u = X \) and its solution \( u = \psi(x, D)X \), the letter \( x \) be changed into the operative symbol \( D \) and \( D \) into \(-x\), we shall thus
obtain another linear differential expression $\varphi(D,-x)u=X$, the solution of which will be $u=\psi(D,-x)X$.

In the application of this theorem, care must be taken that the first-mentioned solution is so written that the operations included under the function $\psi$ are not suppressed; and it must also be borne in mind that the expressions obtained by this interchange of symbols will not in all cases be obviously interpretable.

For applications of this singular analytical process I beg leave to refer to the memoir above cited, where it is employed for the solution, in finite terms, of extensive classes of linear differential equations, and equations in finite differences. So far as the process is legitimate, it is to be observed that it is founded on reasoning of a purely analytical character. It does not in any manner whatever flow from the calculus of operations, or depend for its validity upon the soundness of the logical basis on which this calculus rests.

Now it is a remarkable property of this mechanical interchange of symbols, that it instantaneously converts a linear equation in finite differences into a linear differential equation; so that wherever the former is soluble, the latter is soluble also, provided the result be intelligible, a condition always satisfied when the functions employed are rational algebraical functions.

As an instance worthy of notice, let us take the example last above given (Ex. 6.). Bearing in mind that $u_{x-n}=\varepsilon^{-nD}u_x$, the proposed interchange gives the equation (writing $\varphi x$ for $P_x$),

$$u+a_1\varphi(D)(\varepsilon^x u)+a_2\varphi(D)\varphi(D-1)(\varepsilon^{2x} u)+\ldots+a_n\varphi(D)\ldots\varphi(D-n+1)(\varepsilon^{nx} u)=G,$$

whose solution, therefore, depends upon that of

$$v-\alpha\varphi(D)(\varepsilon^x v)=\alpha^{n-1}AG,$$

a proposition established by Mr. Boole by the methods of the Calculus of Operations.

I propose, therefore, now to employ this theorem of the interchange of symbols for the purpose of converting the forms of solution, above given, of equations in finite differences into the particular solutions of some general forms of differential equations; viz. those equations whose factors do not contain any irrational or transcendental functions of $x$, or contain them only in the form of series of ascending powers of $x$.

11. Mr. Boole, in his General Method in Analysis, has shown that expressions of this character may be placed in the form

$$f_0(D)u+f_1(D)(\varepsilon^x u)+f_2(D)(\varepsilon^{2x} u)+\ldots=U,$$

by changing the independent variable from $x$ to its logarithm $\theta$, and making use of the relation, ($D$ being $\frac{d}{dx}$),

$$D(D-1)\ldots(D-n+1)u=x^n\left(\frac{d}{dx}\right)^n u.$$
Making use of this relation, we immediately convert our equation, which we assume to be in the form

\[
(a_n + b_n x + \ldots + k_n x^{p-1} + l_n x^p + \ldots) \frac{d^n u}{dx^n} + (a_{n-1} + b_{n-1} x + \ldots + k_{n-1} x^{p-1} + l_{n-1} x^p + \ldots) \frac{d^{n-1} u}{dx^{n-1}} + \ldots + (a_0 + b_0 x + \ldots + k_0 x^{p-1} + l_0 x^p + \ldots) u = G,
\]

into

\[
a_n(D-(D-n+1))u + b_n((D-1)\ldots(D-n))(e^\theta u) + c_n((D-2)\ldots(D-n-1))(e^{2\theta} u) + \ldots + t_n((D-p)\ldots(D-n-p+1))(e^{p\theta} u) + \ldots + a_{n-1}(D-1)\ldots(D-n+1))(e^\theta u) + b_{n-1}((D-2)\ldots(D-n))(e^{2\theta} u) + \ldots + k_{n-1}((D-p)\ldots(D-n-p+2))(e^{p\theta} u) + \ldots + a_{n-2}((D-2)\ldots(D-n+1))(e^{2\theta} u) + \ldots + k_{n-2}((D-p)\ldots(D-n-p+3))(e^{p\theta} u) + \ldots = e^{m\theta} G. \quad (2.)
\]

or

\[
a_n u + \left( \frac{b_n(D-n)+a_{n-1}}{D} \right)(e^\theta u) + \left( \frac{c_n(D-n)(D-n-1)+b_{n-1}(D-n)+a_{n-2}}{D(D-1)} \right)(e^{2\theta} u) + \ldots + \left( \frac{d_n(D-n)(D-n-1)(D-n-2)+c_{n-1}(D-n)(D-n-1)+b_{n-2}(D-n)+a_{n-3}}{D(D-1)(D-2)} \right)(e^{3\theta} u) + \ldots = (D-(D-n+1))^{-1}(e^{m\theta} G).
\]

Now if in this equation we change \( D \) into \( \theta \) and \( \theta \) into \(-D\), we obtain the equation in finite differences (which suppose to be of the \( n \)th order),

\[
a_n u_\theta + \frac{b_n(\theta-n)+a_{n-1}}{\theta} u_{\theta-1} + \frac{c_n(\theta-n)(\theta-n-1)+b_{n-1}(\theta-n)+a_{n-2}}{\theta(\theta-1)} u_{\theta-2} + \ldots = (\theta(\theta-1)\ldots(\theta-n+1))^{-1}G_{\theta-n},
\]

or

\[
u_\theta + f_1 \theta u_{\theta-1} + f_2 \theta u_{\theta-2} + \ldots = (f_0 \theta)^{-1} G_{\theta-n} = H_{\theta-n} \quad \text{(suppose)};
\]

the solution of which, by section 8, is of the form

\[
u_\theta = M_0 H_{\theta-n} + M_1 H_{\theta-n-1} + M_2 H_{\theta-n-2} + \ldots + M_m H_{\theta-n-m} + \ldots,
\]

where \( M_0 = 1 \),

\[
M_1 + \frac{f_1 \theta}{f_0 \theta} M_0 = 0,
\]

\[
M_2 + \frac{f_1(\theta-1)}{f_0(\theta-1)} M_1 + \frac{f_2 \theta}{f_0 \theta} M_0 = 0,
\]

\[
\ldots
\]

\[
M_m + \frac{f_1(\theta-m+1)}{f_0(\theta-m+1)} M_{m-1} + \frac{f_2(\theta-m+2)}{f_0(\theta-m+2)} M_{m-2} + \ldots + \frac{f_r(\theta-m+r)}{f_0(\theta-m+r)} M_{m-r} = 0.
\]

Restoring the symbols, and thereby converting \( H_{\theta-n} \) into \((f_0(D))^{-1}(e^{m\theta} G)\), which call \( H \), we have

\[
u = M_0 H + M_1 (e^\theta H) + M_2 (e^{2\theta} H) + \ldots + M_m (e^{m\theta} H) + \ldots,
\]

where \( M_0, M_1, \ldots \) denote a series of operations having the following significations and relations;

\[
M_0 = 1,
\]

\[
M_1 + \frac{f_1(D)}{f_0(D)} M_0 = 0,
\]
\[ M_2 + \frac{f_1(D-1)}{f_0(D-1)} M_1 + \frac{f_2(D)}{f_0(D)} M_0 = 0, \]

\[ \vdots \quad \vdots \quad \vdots \]

\[ M_m + \frac{f_1(D-m+1)}{f_0(D-m+1)} M_{m-1} + \frac{f_2(D-m+2)}{f_0(D-m+2)} M_{m-2} + \cdots + \frac{f_r(D-m+r)}{f_0(D-m+r)} M_{m-r} = 0; \]

or, by passing \( \varepsilon^\theta, \varepsilon^{2\theta}, \) &c. outside the operations by the equation

\[ \varphi(D)(\varepsilon^{m\theta} H) = \varepsilon^{m\theta} \varphi(D+m) H, \]

\[ u = M_0 H + \varepsilon^\theta M_1 H + \varepsilon^{2\theta} M_2 H + \cdots + \varepsilon^{m\theta} M_m H + \cdots, \ldots \ldots \ldots (3.) \]

where the general law of relation is

\[ M_m + \frac{f_1(D+1)}{f_0(D+1)} M_{m-1} + \frac{f_2(D+2)}{f_0(D+2)} M_{m-2} + \cdots + \frac{f_r(D+r)}{f_0(D+r)} M_{m-r} = 0. \]

Now the expression \( H, \) which is the subject of all the operations, contains \( n \) arbitrary constants, since \( f_0(D) \) is of the \( n \)th order; that is, provided \( a_n \) is not zero.

Let \( \beta_1 \beta_2 \ldots \beta_n \) be the roots of \( f_0 t = 0; \) then the complete value of \( H \) is

\[ (f_0(D))^{-1}(\varepsilon^{m\theta} G) + c_1 \varepsilon^{\beta_1\theta} + c_2 \varepsilon^{\beta_2\theta} + \cdots + c_n \varepsilon^{\beta_n\theta}; \]

of which the first term will give us the particular solution of the original equation, and each of the other terms a complementary solution. Bearing in mind the equation \( \varphi(D)(\varepsilon^{m\theta}) = \varphi p . \varepsilon^{p\theta}, \) we see at once that the first complementary solution is

\[ c_1 \varepsilon^{\beta_1\theta}(A_0 + A_1 x + A_2 x^2 + \cdots + A_m x^m + \cdots), \]

or

\[ c_1 x^{\beta_1}(A_0 + A_1 x + A_2 x^2 + \cdots + A_m x^m + \cdots), \]

where \( A_0 = 1, \) and the law of formation of the coefficients is

\[ f_0(\beta_1 + m) A_m + f_1(\beta_1 + m) A_{m-1} + f_2(\beta_1 + m) A_{m-2} + \cdots + f_r(\beta_1 + m) A_{m-r} = 0; \]

and the remaining complementary solutions merely require the substitution of \( \beta_2 \ldots \beta_n \) successively for \( \beta_1, \) with new constants.

The reader will not fail to perceive the peculiarity of the series when \( a_n \) is unity, (which value it can always have when it is not zero); for in that case the roots of \( f_0 t = 0 \) are the natural numbers from 0 to \( n - 1 \) inclusive, so that the series begin respectively with the terms 1, \( x, x^2, \ldots x^{n-1}. \)

If \( a_n \) be zero, that is, if the factor of the highest differential coefficient of \( u \) do not contain an absolute term, then in order that the transformed equation may take the form (2.), it will be necessary to pass \( \varepsilon^\theta \) outside the operative functions in each term, and divide by \( \varepsilon^\theta. \) The initial function \( f_0 t \) will then be of the form

\[ b_n(t \ldots (t-n+1)) + a_{n-1}(t \ldots (t-n+2)), \]

and \( H \) will be

\[ (f_0(D))^{-1}(\varepsilon^{(n-1)\theta} G). \]

Similarly, if in addition to \( a_n \) being zero, we have also \( b_n \) and \( a_{n-1} \) respectively equal to zero, it will be necessary to pass \( \varepsilon^{2\theta} \) outside the operative functions, and divide by \( \varepsilon^\theta. \)
and so on if other terms of the factors are wanting. Thus we can in all cases obtain the transformed equation in the required shape; but in all the cases where coefficients thus vanish, there is the important qualification that \( f_0 \) is no longer necessarily of the \( n \)th order; so that \( H \) does not necessarily contain the proper number of arbitrary constants. The consequences of this consideration will be afterwards developed; in the meantime we proceed to consider what modifications the series undergo where \( f_0 \) has two or more equal roots.

12. If there should be two roots of \( f_0 = 0 \) equal to \( \beta_1 \), one series will be deficient; and it will be supplied as follows. The expression \( H \), or \( (f_0(D))^{-1}(\varepsilon^{\beta_1}G) \), contains in that case a term of the form \( k_1 \theta \varepsilon^{\beta_1} \); and it is easily seen that

\[
\varphi(D)(\theta \varepsilon^{\beta_1}) = \theta \varphi(D)(\varepsilon^{\beta_1}) + \varphi'(D)(\varepsilon^{\beta_1}).
\]

The wanting series is, therefore,

\[
k_1 x^{\beta_1} \{ \log x(1 + A_1 x + A_2 x^2 + ..) + (A'_1 x + A'_2 x^2 + ..) \},
\]

where

\[
A'_1 = \frac{dA_1}{d\beta_1} \text{ &c.}
\]

If there should be three roots of \( f_0 = 0 \) equal to \( \beta_1 \), two series will be deficient, one of which will be supplied as last mentioned; and the other by the introduction of the series

\[
k_2 x^{\beta_1} \{ (\log x)^2(1 + A_1 x + A_2 x^2 + ..) + 2 \log x(A'_1 x + A'_2 x^2 + ..) + (A''_1 x + A''_2 x^2 + ..) \},
\]

where \( A''_1 = \frac{d^2A_1}{d\beta_1^2} \text{ &c.}; \) for we have a term \( k_2 \theta^2 \varepsilon^{\beta_1} \); and it is easily seen that

\[
\varphi(D)(\theta^2 \varepsilon^{\beta_1}) = \theta^2 \varphi(D)\varepsilon^{\beta_1} + 2 \theta \varphi'(D)\varepsilon^{\beta_1} + \varphi''(D)\varepsilon^{\beta_1}.
\]

And generally since, where there are \( p+1 \) equal roots \( \beta_1 \), we have terms

\[
c_1 \varepsilon^{\beta_1} + k_1 \theta \varepsilon^{\beta_1} + k_2 \theta^2 \varepsilon^{\beta_1} + .. + k_p \theta^p \varepsilon^{\beta_1};
\]

and since

\[
\varphi(D)(\theta^p \varepsilon^{\beta_1}) = \theta^p \varphi(D)\varepsilon^{\beta_1} + p \theta^{p-1} \varphi'(D)\varepsilon^{\beta_1} + p \frac{p-1}{2} \theta^{p-2} \varphi''(D)\varepsilon^{\beta_1} + ..;
\]

the deficient series will be supplied by the following, taken with a different constant for every value of \( p \) from unity upwards:

\[
k_p x^{\beta_1} \{ (\log x)^p(1 + A_1 x + A_2 x^2 + ..) + p(\log x)^{p-1}(A'_1 x + A'_2 x^2 + ..)
\]

\[
+ p \frac{p-1}{2} (\log x)^{p-2}(A''_1 x + A''_2 x^2 + ..) + .. + p \log x(A^{(p-1)}_1 x + A^{(p-1)}_2 x^2 + ..) + (A^{(p)}_1 x + A^{(p)}_2 x^2 + ..) \}.
\]

As a matter of convenience, when the equal roots are zero, a temporary nominal value should be given to them for the purpose of differentiating \( A_1 \), &c.

13. Hitherto we have attended especially to the complementary solutions, or in other words, regarded \( G \) as zero. The operations, however, indicated in (3.) may be readily performed when \( G \) is a rational function of \( x \); which we will suppose to be
cleared of fractions, and so cleared to consist of a set of terms \( \alpha_p x^p \). Then for this term

\[
(f_0(D))^{-1}(\varepsilon^{nG}) = \alpha_p (f_0(D))^{-1} \varepsilon^{(n+p)\theta} = \alpha_p (f_0(n+p))^{-1} \varepsilon^{(n+p)\theta};
\]

and we have, as before,

\[
u = \alpha_p x^{(n+p)}(B_0 + B_1 x + B_2 x^2 + \ldots + B_m x^m + \ldots),
\]

where \( B_0 = 1 \), and the law of formation is

\[
f_0(n+p+m)B_m + f_1(n+p+m)B_{m-1} + f_2(n+p+m)B_{m-2} + \ldots = 0;
\]

and the whole solution is found by taking all the values of \( p \). This will undergo a slight alteration, as the incipient term will be of the form \( f_0(D)(\varepsilon^{(n-r)\theta}G) \), where \( r \) factors of (2.) vanish by reason of some of the constant coefficients \( a_n \), &c. being zero.

In like manner it would be easy to represent the series if \( G \) contained \( \log x \) and its powers; but for most other forms it would be necessary to expand \( G \) in order to represent the series explicitly. The solution however is theoretically complete, since it consists solely in the performance of operations which are known explicit functions of \( D \).

14. Before proceeding further with the main subject, I shall illustrate this process by a few examples.

Ex. 1. Let the equation be

\[
(1 + b_2 x + c_2 x^2) \frac{d^2 u}{dx^2} + (a_1 + b_1 x) \frac{du}{dx} + a_0 u = G.
\]

Referring to (2.), we have

\[
f_0D = D(D-1), \quad f_1D = b_2(D-1)(D-2) + a_1(D-1),
\]

\[
f_2D = c_2(D-2)(D-3) + b_1(D-2) + a_0;
\]

roots of \( f_0t = 0 \) are 0 and 1.

First complementary series,

\[
c_1(A_0 + A_1 x + A_2 x^2 + \ldots + A_m x^m + \ldots),
\]

where

\[
A_0 = 1, \quad m(m-1)A_m + (b_2(m-1)(m-2) + a_1(m-1))A_{m-1} + (c_2(m-2)(m-3) + b_1(m-2) + a_0)A_{m-2} = 0
\]

is the law of formation.

Second complementary series,

\[
c_2x(A_0 + A_1 x + A_2 x^2 + \ldots + A_m x^m + \ldots),
\]

where

\[
A_0 = 1, \quad (m+1)mA_m + (b_2(m)(m-1) + a_1m)A_{m-1} + (c_2(m-1)(m-2) + b_1(m-1) + a_0)A_{m-2} = 0
\]

is the law of formation.

Particular solution,

\[
u = \psi_0(D)H + \varepsilon^{nG}\psi_1(D)H + \varepsilon^{2nG}\psi_2(D)H + \ldots + \varepsilon^{mG}\psi_m(D)H + \ldots,
\]

where

\[
H = \left( \frac{d}{dx} \right)^{-2} G, \quad \psi_0(D) = 1, \quad \text{and} \quad f_0(D+m)\psi_0(D) + f_1(D+m)\psi_1(D) + f_2(D+m)\psi_2(D) = 0
\]

is the law of the formation of the operative functions.
The three laws of formation, which are here written down at length, are in substance the same; for from the last the others are made by merely writing the roots successively in lieu of D in the factors.

Ex. 2. Let the equation be

\[(x^2 + qx^4) \frac{d^2u}{dx^2} + (x + px^3 + 5qx^3) \frac{du}{dx} + (px - n^2 + (4q + r)x^3)u = G.\]

Referring to (2.), we have in the first instance,

\[(D^2 - n^2)(\varepsilon^2 u) + p(D - 2)(\varepsilon^2 u) + (q(D - 4)D + 4q + r)\varepsilon^2 u = G,\]

whence

\[(D^2 - n^2)u + pD(\varepsilon^2 u) + (qD^2 + r)(\varepsilon^2 u) = \varepsilon^{-2}G;\]

roots of \(f_0 t = 0\) are \(n\) and \(-n\).

First complementary series,

\[c_1 x^n(A_0 + A_1 x + A_2 x^2 + \ldots + A_m x^m + \ldots),\]

where

\[A_0 = 1,\] and \[(n + m)^2 - n^2)A_m + p(n + m)A_{m-1} + (q(n + m)^2 + r)A_{m-2} = 0\]

is the law of formation.

Second complementary series,

\[c_2 x^{-n}(A_0 + A_1 x + A_2 x^2 + \ldots + A_m x^m + \ldots),\]

where the law is, as before, changing the sign of \(n\).

Particular solution,

\[u = \psi_0(D)H + \varepsilon^2 \psi_1(D)H + \ldots + \varepsilon^{m-2} \psi_m(D)H + \ldots,\]

where

\[H = (D^2 - n^2)^{-1}(\varepsilon^{-2}G),\] \[\psi_0(D) = 1,\]

and

\[(D + m)^2 - n^2)\psi_m(D) + p(D + m)\psi_{m-1}(D) + (q(D + m)^2 + r)\psi_{m-2}(D) = 0\]

is the law of the formation of the operative functions.

If \(p = 0, q = 0,\) and \(r = 1,\) so that the equation becomes

\[x^2 \frac{d^2u}{dx^2} + x \frac{du}{dx} + (x^2 - n^2)u = 0,\]

the law is

\[A_m = -\frac{A_{m-2}}{m(2n + m)},\]

whence

\[u = c_1 x^n \left[1 - \frac{1}{4}(1 + n)^{-1}x^2 + \frac{1}{4.8}(1 + n)^{-1}(2 + n)^{-1}x^4 - \frac{1}{4.8.12}(1 + n)^{-1}(2 + n)^{-1}(3 + n)^{-1}x^6 + \ldots\right],\]

\[+ c_2 x^{-n} \left[1 - \frac{1}{4}(1 - n)^{-1}x^2 + \frac{1}{4.8}(1 - n)^{-1}(2 - n)^{-1}x^4 - \frac{1}{4.8.12}(1 - n)^{-1}(2 - n)^{-1}(3 - n)^{-1}x^6 + \ldots\right].\]

Ex. 3. Let the equation be

\[x^3 \frac{d^3u}{dx^3} + 3x^2 \frac{d^2u}{dx^2} + x \frac{du}{dx} + qx^nu = G.\]
Referring to (2.), we have

\[ D^3 u + q \varepsilon^n u = G; \]

\[ f_0(D) = D^3; \text{ three roots equal to zero;} \]

\[ f_n(D) = q; \]

\[ m^3 A_m + q A_{m-n} = 0, \text{ law of formation.} \]

Hence the complementary series are,

\[ u = c_1 \left\{ 1 - \frac{q}{n^3} x^n + \frac{q}{n^3 (2n)^3} x^{2n} - \frac{q}{n^3 (2n)^3 (3n)^3} x^{3n} + \ldots \right\} \]

\[ + c_2 \left\{ \log x \left( 1 - \frac{q}{n^3} x^n + \frac{q}{n^3 (2n)^3} x^{2n} - \frac{q}{n^3 (2n)^3 (3n)^3} x^{3n} + \ldots \right) \right. \]

\[ + 3 \left( \frac{1}{n} \frac{q}{n^3} x^n - \left( \frac{1}{n} + \frac{1}{2n} \right) \frac{q}{n^3 (2n)^3} x^{2n} + \left( \frac{1}{n} + \frac{1}{2n} + \frac{1}{3n} \right) \frac{q}{n^3 (2n)^3 (3n)^3} x^{3n} + \ldots \right) \}

\[ + c_3 \left\{ (\log x)^2 \left( 1 - \frac{q}{n^3} x^n + \frac{q}{n^3 (2n)^3} x^{2n} - \frac{q}{n^3 (2n)^3 (3n)^3} x^{3n} + \ldots \right) \right. \]

\[ + 6 \log x \left( \frac{1}{n} \frac{q}{n^3} x^n - \left( \frac{1}{n} + \frac{1}{2n} \right) \frac{q}{n^3 (2n)^3} x^{2n} + \left( \frac{1}{n} + \frac{1}{2n} + \frac{1}{3n} \right) \frac{q}{n^3 (2n)^3 (3n)^3} x^{3n} + \ldots \right) \]

\[ + 6 \left( -\frac{2}{n^3} \frac{q}{n^3} x^n + \left( \frac{2}{n^2} + \frac{2}{(2n)^2} + \frac{3}{n(2n)} \right) \frac{q}{n^3 (2n)^3} x^{2n} \right. \]

\[ - \left( \frac{2}{n^2} + \frac{2}{(2n)^2} + \frac{3}{n(2n)} + \frac{3}{n(3n)} + \frac{3}{(2n)(3n)} \right) \frac{q}{n^3 (2n)^3 (3n)^3} x^{3n} + \ldots \right) \};

and the particular solution is

\[ u = H - qx^n(D+n)^{-3}H + q^2x^{2n}(D+n)^{-3}(D+2n)^{-3}H - \ldots, \]

\( H \) being \( D^{-3}(\varepsilon^n G) \).

15. The completeness of the preceding forms of solution depends, as above intimated, upon the circumstance that the function \( f_0(D) \) is of an order not lower than the order of the original equation. It may however be of a lower order, as would take place in the first of the examples above given, if \( a_2 \) instead of being 1 were zero and \( b_2 \) were also zero.

Let the equation then be (Ex. 4.), 

\[ c_2 x^2 \frac{d^2 u}{dx^2} + (1 + b_1 x) \frac{du}{dx} + a_0 u = G. \]

Referring to (2.), we have, in the first instance,

\[ (D-1)(\varepsilon^n u) + (c_2(D-2)(D-3) + b_1(D-2) + a_0)(\varepsilon^n u) = G, \]

or

\[ Du + (c_2(D-1)(D-2) + b_1(D-1) + a_0)(\varepsilon^n u) = \varepsilon^{-n} G; \]

so that \( f_0(D) \) is of the first order, and the root is zero.
The only complementary function to be obtained is therefore

\[ u = c_1 \left[ 1 - a_0 x + \frac{1}{2} a_0 (b_1 + a_0) x^2 - \frac{1}{2 \cdot 3} a_0 (b_1 + a_0)(1.2c_2 + 2b_1 + a_0)x^3 \right. \\
+ \frac{1}{2 \cdot 3 \cdot 4} a_0 (b_1 + a_0)(1.2c_2 + 2b_1 + a_0)(2.3c_2 + 3b_1 + a_0)x^4 - \ldots \bigg], \]

a series which is divergent, but which, as will also be seen afterwards, is finite when the constant coefficients are connected by the formula \((p-1)p c_2 + p b_1 + a_0 = 0\), \(p\) being any positive integer.

Further, it may happen that the series obtained by the process, even when they afford a complete solution in respect of the number of constants, are divergent for all or for some values of \(x\). This may evidently be the case in the solution of the second of the examples above given; for the law of the coefficients, as we advance in the series, approximates to \(A_m = -q A_{m-2}\).

In these cases, other solutions in series may be obtained by resolving the equation in finite differences in a series of terms of the form \(H_{\theta+p}\) instead of \(H_{\theta-p}\). In order to effect this, all that is necessary is to write \(\theta + r\) for \(\theta\), \((r\) being the order of the equation in finite differences), and to divide by the factor of the last term instead of the factor of the first term; or in other words, we must pass \(\varepsilon^{-\theta}\) outside the functions in (2.), and multiply by \(\varepsilon^{-\theta}\); so that this equation now assumes the form

\[ f_r(D)u + f_{r-1}(D)(\varepsilon^{-\theta}u) + f_{r-2}(D)(\varepsilon^{-2\theta}u) + \ldots = \varepsilon^{(n-r)\theta}G; \]

and the equation in finite differences is

\[ f_r(\theta)u_\theta + f_{r-1}(\theta)u_{\theta+1} + f_{r-2}(\theta)u_{\theta+2} + \ldots = G_{\theta+r-n}. \]

We have now to inquire for the roots of \(f_r t = 0\); the incipient term is \((f_r(D))^{-1}(\varepsilon^{(n-r)\theta}G)\), which call \(H_1\); and the particular solution will be found to be

\[ u = \psi_0(D)H_1 + \varepsilon^{-\theta}\psi_1(D)H_1 + \varepsilon^{-2\theta}\psi_2(D)H_1 + \ldots + \varepsilon^{-m\theta}\psi_m(D)H_1 + \ldots, \]

where

\[ \psi_0(D) = 1, \text{ and } f_r(D-m)\psi_m(D) + f_{r-1}(D-m)\psi_{m-1}(D) + \ldots = 0 \]

is the law of the formation of the operative functions.

The substitution of the roots of \(f_r t\) successively for \(D\) gives the law of formation of the complementary series.

Taking the last example, the transformed equation now becomes

\[ (c_2 D(D-1) + b_1 D + a_0)u + (D+1)(\varepsilon^{-\theta}u) = \varepsilon^{-2\theta}G. \]

Let the roots of \(f_r t\) or \(c_r(t-1) + b_t + a_0 = 0\), be \(\beta_1\) and \(\beta_2\).

The first complementary series is

\[ c_1 x^{\beta_1}(A_0 + A_1 x^{-1} + \ldots + A_m x^{-m} + \ldots), \]
where $A_0 = 1$, and $f_r(\beta_1 - m)A_m + (\beta_1 - m + 1)A_{m-1} = 0$ is the law of formation; the series then are

$$c_1 \left\{ x^{\beta_1} - \frac{\beta_1}{f_r(\beta_1 - 1)} x^{\beta_1 - 1} + \frac{\beta_1 - 1}{f_r(\beta_1 - 2)} x^{\beta_1 - 2} - \frac{\beta_1 - 2}{f_r(\beta_1 - 3)} x^{\beta_1 - 3} + \ldots \right\}$$

$$+ c_2 \left\{ \text{similar function of } \beta_2 \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \right\},$$

which is, (since $f_r(\beta_1 - n) = n(n + 1)c_2 - 2n\beta_1 - nb_1$),

$$c_1 \left\{ x^{\beta_1} - \frac{\beta_1}{2c_2 - 2\beta_1 - b_1} x^{\beta_1 - 1} + \frac{\beta_1 - 1}{2c_2 - 2\beta_1 - b_1} x^{\beta_1 - 2} - \frac{\beta_1 - 2}{2c_2 - 2\beta_1 - b_1} x^{\beta_1 - 3} + \ldots \right\}$$

$$+ c_2 \left\{ \text{similar function of } \beta_2 \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \right\}.$$  

When $\beta_1$ or $\beta_2$ is a positive integer, one of these series is terminable; and if both are positive integers, the series derived from the smaller root is terminable, and the other gives no result, the coefficients becoming infinite. The first of the series will then be found to give the same result as that produced from the divergent series (which is evidently terminable in form in the case indicated), except that it begins at the other end. In this case the other complementary solution can be found in finite terms by reducing the order of the equation.

The particular solution is

$$u = H_1 - x^{-1} \frac{D}{f_r(D - 1)} H_1 + x^{-2} \frac{D}{f_r(D - 1)} \frac{D - 1}{f_r(D - 2)} H_1 - \ldots,$$

$H_1$ being $(f_r(D))^{-1}(\varepsilon^{-29}G)$.

Let us now return to example 2, the solution of which, as above found, is in some cases divergent.

The transformed equation now becomes

$$(q(D + 2)^2 + r)u + p(D + 2)(\varepsilon^{-6}u) + ((D + 2)^2 - r^2)(\varepsilon^{-40}u) = \varepsilon^{-40}G.$$

Roots of $f_r(t) = 0$ are $-2 \pm \sqrt{\frac{r}{q}}$, which call $\beta_1$ and $\beta_2$.

First complementary series,

$$c_1 x^{\beta_1} \left( A_0 + \frac{A_1}{x} + \frac{A_2}{x^2} + \ldots + \frac{A_m}{x^m} + \ldots \right),$$

where

$A_0 = 1$, and $(g(\beta_1 - m + 2)^2 + r)A_m + p(\beta_1 - m + 2)A_{m-1} + ((\beta_1 - m + 2)^2 - r^2)A_{m-2} = 0,$

is the law of formation; and the second complementary series is the same, using the other root with a different constant.

In like manner the particular solution is easily represented.

16. We are now in a position to discuss the character of the various series obtained by this process with reference to their convergency or divergency, a subject of the highest importance to the value of the process. The investigations which follow, will, it is apprehended, be found to afford a complete test of the nature of the series.
In general, it will of course be understood that these researches, as to convergency and divergency, relate only to the complementary series, or in other words, to equations deprived of their second member; nevertheless they will to a great extent, if not throughout, apply to cases in which the second member is in the form of integer powers of \( x \) or of \( \log x \).

1st. When in the set of functions

\[ f_0(D), f_1(D), f_2(D), \ldots f_{r-1}(D), f_r(D), \]

the function \( f_0(D) \) is of a higher dimension with regard to \( D \) than any other of the set, or is what we shall here call the dominant function, the solution of the equation can always be found in a convergent series of ascending powers of \( x \); and if in such a case we solve the equation in a series of descending powers of \( x \), which we can do if we please, that series is certainly divergent.

This is immediately apparent from the consideration of the law of the coefficients

\[ f_0(\beta_1 + m)A_m + f_1(\beta_1 + m)A_{m-1} + \cdots + f_r(\beta_1 + m)A_{m-r} = 0; \]

which, as \( m \) increases without limit, approaches to the form

\[ A_m = -\frac{f_{r,m}}{f_{0,m}}A_{m-1} - \frac{f_{r,m}}{f_{0,m}}A_{m-2} - \ldots - \frac{f_{r,m}}{f_{0,m}}A_{m-r}, \text{ or } mA_m = -a_{n-1}A_{m-1} - b_{n-1}A_{m-2} - \ldots, \]

assuming that all the functions are only one degree lower than \( A_m \), which is the least favourable case for convergency. Therefore, if the largest of the terms of the right-hand side of this equation be \( l_{n-1}A_{m-p} \), we have

\[ A_m < \frac{n_{l_{n-1}}}{m}A_{m-p}; \]

and we can therefore arrive at a point in the series at which the ratio of the coefficient of \( x^m \) to that of \( x^{m-p} \) diminishes without limit.

It will also be observed that the series introduced by two or more equal roots are of the same character as the original series from which they are derived; for, \( A_m \) being of the form \( \phi(\beta_1 + m) \), we have \( \frac{dA_m}{d\beta_1} = \frac{dA_m}{dm} \); and when \( A_{m-p} \) is of a higher order with reference to \( m \) than \( A_m \), \( \frac{dA_{m-p}}{dm} \) is also of a higher order than \( \frac{dA_m}{dm} \), and so for the other differential coefficients.

We have now merely to inquire what must be the form of the original equation that \( f_0(D) \) may be the dominant function. Referring to (2.), we see that it is necessary that the factor of the highest differential coefficient of \( u \) should contain one term only. If this factor be 1 or \( x \), no restriction need be imposed on the succeeding factors. If it be \( x^2 \), the factor of the next lower differential coefficient must not contain an absolute term; and generally, if \( x^p \) be the factor of \( \frac{d^n u}{dx^n} \), the factor of \( \frac{d^{n-1} u}{dx^{n-1}} \) must begin with a term not lower than \( x^{p-1} \), that of \( \frac{d^{n-2} u}{dx^{n-2}} \) with a term not lower than \( x^{p-2} \), and so on.
In short, all equations where \( x \) does not enter into the factor of \( \frac{d^n u}{dx^n} \), and all equations of the form

\[
x^p \frac{d^n u}{dx^n} + (k_n x^{p-1} + l_n x^p + \ldots) \frac{d^{n-1} u}{dx^{n-1}} + (h_n x^{p-2} + \ldots) \frac{d^{n-2} u}{dx^{n-2}} + \ldots = 0,
\]

where \( p \) is a positive integer, are soluble in convergent series of ascending powers of \( x \). The third example above given is an instance of this form.

2ndly. When in the set of functions

\[
f_r(D), f_{r-1}(D), \ldots f_1(D), f_0(D),
\]

the function \( f_r(D) \) is the dominant function, the solution of the equation can always be found in a convergent series of descending powers of \( x \); and if in such a case we solve the equation in a series of ascending powers of \( x \), that series is certainly divergent.

This is apparent, as before, from the consideration of the law of the coefficients,

\[
f_r(\beta_i - m) A_m + f_{r-1}(\beta_i - m) A_{m-1} + \ldots + f_0(\beta_i - m) A_{m-r} = 0,
\]

which, as \( m \) increases without limit, approaches to

\[
mA_m = h_{n-1} A_{m-1} - k_{n-1} A_{m-2} + \ldots
\]

in the least favourable case for convergency.

On proceeding to inquire what must be the form of the original equation, we see again that it is necessary that the factor of the highest differential coefficient of \( u \) should contain one term only. If this be \( x^p \), then the other restrictions are, that the factor of the next differential coefficient must stop at \( x^{p-1} \), that of the next at \( x^{p-2} \), and so on.

In short, for all integer values of \( p \) the equation

\[
x^p \frac{d^n u}{dx^n} + (a_{n-1} + b_{n-1} x + \ldots + k_n x^{p-1}) \frac{d^{n-1} u}{dx^{n-1}} + (a_{n-2} + b_{n-2} x + \ldots + h_{n-2} x^{p-2}) \frac{d^{n-2} u}{dx^{n-2}} + \ldots = 0
\]

is soluble in a convergent series of descending powers of \( x \). The 4th example above given is an instance of this form.

3rdly. When in the set of functions

\[
f_0(D), f_1(D), \ldots f_{r-1}(D), f_r(D),
\]

the functions \( f_0(D) \) and \( f_r(D) \) are of the same dimensions, and are both dominant over all the other functions, the solution of the equation can be found in a series of ascending powers of \( x \), which for some values of \( x \) is convergent, and for other values of \( x \) is divergent; and the solution can also be found in a series of descending powers of \( x \) which is divergent for all values of \( x \) for which the other series is convergent, and convergent for all values of \( x \) for which the other series is divergent.

For in the ascending series the law of the coefficients approaches the form, (the
coefficient of the highest power of \( m \) in \( f_r m \) being 1, and that in \( f_r m \) being \( l_n \),

\[
A_m = -l_n A_{m-r},
\]

and in the descending series the law approaches to

\[
l_n A_m = \pm A_{m-r}.
\]

The former series is therefore convergent for all values of \( x \) numerically less than \((l_n)^{-\frac{1}{r}}\), and divergent for all values of \( x \) numerically greater than this limit; and the latter series is divergent for all values of \( x \) numerically greater than this quantity, and divergent for all values of \( x \) numerically less.

The equations to which this rule is applicable are of the forms, \((p'\) being less than \(p)\)

\[
(x^{p'} + l_n x^p) \frac{d^n u}{dx^n} + (h_{n-1} x^{p'-1} + \ldots + k_{n-1} x^{p-1}) \frac{d^{n-1} u}{dx^{n-1}} + (g_{n-2} x^{p'-2} + \ldots + h_{n-2} x^{p-2}) \frac{d^{n-2} u}{dx^{n-2}} + \ldots = 0,
\]

and

\[
(a_n + l_n x^p) \frac{d^n u}{dx^n} + (a_{n-1} + b_{n-1} x + \ldots + k_{n-1} x^{p-1}) \frac{d^{n-1} u}{dx^{n-1}} + (a_{n-2} + \ldots + h_{n-2} x^{p-2}) \frac{d^{n-2} u}{dx^{n-2}} + \ldots = 0.
\]

The second example above given is an instance of the first of these forms.

4thly. When in the set of functions

\[
f_0(D), \ f_1(D), \ldots f_{r-1}(D), \ f_r(D),
\]

one or more of the intermediate functions is or are of the same order as the extreme functions \( f_0(D) \) and \( f_r(D) \), or as the highest of these two when they differ in dimensions, the series obtained by the above processes will be divergent for some values of \( x \), and we have not as yet any method of deriving a convergent series corresponding to these values; and if one or more of the intermediate functions be of a higher dimension than the extreme functions, the series obtained by the above processes will certainly be divergent.

These remaining cases therefore sever into two species; first, where some of the intermediate functions are of the same order as the highest of the extreme functions; secondly, where one or more of the intermediate functions are dominant.

The first of these species includes equations of the two following forms:

\[
(a_n x^{p'} + b_n x^{p'+1} + \ldots + l_n x^p) \frac{d^n u}{dx^n} + (a_{n-1} x^{p'-1} + b_{n-1} x^{p'} + \ldots) \frac{d^{n-1} u}{dx^{n-1}} + \ldots + (a_0 x^{p-n} + \ldots) u = 0,
\]

in which there can be no function higher than \( f_0(D) \); and

\[
(a_n + b_n x + \ldots + l_n x^p) \frac{d^n u}{dx^n} + (a_{n-1} + b_{n-1} x + \ldots + k_{n-1} x^{p-1}) \frac{d^{n-1} u}{dx^{n-1}} + \ldots = 0,
\]

in which there can be no function higher than \( f_r(D) \).

In these cases it will easily be seen that the law of the coefficients of the ascending series, as \( m \) increases without limit, approximates to

\[
a_n A_m + b_n A_{m-1} + \ldots + l_n A_{m-p} = 0,
\]
and the series therefore approaches without limit to a recurring series, in which the constants of relation are

\[-\frac{b_n}{a_n}x, -\frac{c_n}{a_n}x^2, \ldots -\frac{l_n}{a_n}x^p;\]

and the denominator of the rational fraction, to which the residue of the series approaches, is

\[\frac{1}{a_n}(a_n + b_nx + \ldots + l_nx^p).\]

In the descending series, the law of the coefficients approximates to

\[l_nA_m + k_nA_{m-1} + \ldots + a_nA_{m-p} = 0,\]

and the series approaches without limit to a recurring series, in which the constants of relation are

\[-\frac{k_n}{l_n}\frac{1}{x}, \ldots -\frac{b_n}{l_n}\frac{1}{x^{p-1}} - \frac{a_n}{l_n}\frac{1}{x^p};\]

and the denominator of the rational fraction, to which the residue of the series approaches, is

\[\frac{1}{l_nx^p}(a_n + b_nx + \ldots + l_nx^p).\]

When \(f_o(D)\) is higher than \(f_r(D)\), the ascending series alone can be used; when \(f_r(D)\) is higher than \(f_o(D)\), the descending series alone can be used; and when \(f_o(D)\) and \(f_r(D)\) are of the same dimension, either may be used; and the approximations above referred to render it probable that these series, notwithstanding that they may be divergent, are the developments of continuous algebraical expressions.

The second of the species above referred to includes all equations which are excluded from the preceding forms; that is, all forms which transgress both the restrictions to which the equation in the third case is subjected.

Of these forms, the solutions, whether obtained in ascending or in descending series, are always divergent; and the divergency appears to be of an extreme and unmanageable character. In this case we have an intermediate dominant function; and the convergent solutions might, from considerations of analogy, be presumed to be series infinite in both directions, the roots of the dominant function determining the incipient terms.

The treatment of these forms requires the solution of the equation in finite differences

\[\ldots + R_xu_{x+3} + Q_xu_{x+2} + P_xu_{x+1} + u_x + P_xu_{x-1} + Q_xu_{x-2} + R_xu_{x-3} + \ldots = G_x.\]

not starting from either of the two extreme terms, as is done above, but from the term \(u_x\), so as to get a result in the form

\[u_x = \ldots + M_{-2}G_{x+2} + M_{-1}G_{x+1} + M_0G_x + M_1G_{x-1} + M_2G_{x-2} + \ldots\]

It would probably not be difficult to show that such a solution exists; but I have not found one in a form available for the purpose to which it is desired to be applied.
17. Throughout the preceding investigations, the series obtained by the processes here displayed undergo a modification of form in the event of the expression $f_t$ or $f_t$, as the case may be, having one or more sets of imaginary roots.

Let there be a couple of such roots of the form $\alpha \pm \beta \sqrt{-1}$. In the ascending series these roots give

$$c_1 x^{\alpha + \beta \sqrt{-1}} (A_0 + A_1 x + A_2 x^2 + \ldots + A_m x^m + \ldots)$$

$$c_2 x^{\alpha - \beta \sqrt{-1}} (B_0 + B_1 x + B_2 x^2 + \ldots + B_m x^m + \ldots),$$

where

$$f_0 (\alpha + m + \beta \sqrt{-1}) A_m + f_1 (\alpha + m + \beta \sqrt{-1}) A_{m-1} + \ldots = 0$$

and

$$f_0 (\alpha + m - \beta \sqrt{-1}) B_m + f_1 (\alpha + m - \beta \sqrt{-1}) B_{m-1} + \ldots = 0$$

are the respective laws of formation.

It is apparent, therefore, that if $A_m$ be of the form $\varphi (\alpha + \beta \sqrt{-1})$, $B_m$ is of the form $\varphi (\alpha - \beta \sqrt{-1})$.

Making $c_2 = c_1$, and remembering that

$$x^{\beta \sqrt{-1}} + x^{-\beta \sqrt{-1}} = 2 \cos (\beta \log x)$$

$$x^{\beta \sqrt{-1}} - x^{-\beta \sqrt{-1}} = 2 \sqrt{-1} \sin (\beta \log x),$$

the sum of the two series gives the double series,

$$2c_1 x^\alpha (\cos (\beta \log x) \{A_0 + B_0 + (A_1 + B_1)x + \ldots + (A_m + B_m)x^m + \ldots\}$$

$$+ \sqrt{-1} \sin (\beta \log x) \{A_0 - B_0 + (A_1 - B_1)x + \ldots + (A_m - B_m)x^m + \ldots\});$$

which is necessarily real, since $A_m + B_m$ is purely real, and $A_m - B_m$ is purely imaginary.

Making now $c_1 = -c_2$, the sum of the two series gives another double series, (making $c_2 = -k \sqrt{-1}$),

$$2kx^\alpha (\sqrt{-1} \cos (\beta \log x) \{A_0 - B_0 + (A_1 - B_1)x + \ldots + (A_m - B_m)x^m + \ldots\}$$

$$+ \sin (\beta \log x) \{A_0 + B_0 + (A_1 + B_1)x + \ldots + (A_m + B_m)x^m + \ldots\}),$$

which is likewise real.

The descending series may be treated in a similar manner.

18. Most of the examples to which the preceding processes are applied have been taken from the paper in the Philosophical Transactions for 1844, in which Mr. Boole developed his new General Method in Analysis, with which the subject matter of the present paper is closely connected, though the methods exhibited are distinct; unless indeed it should prove, that the interchange of the symbol of operation and the independent variable, and the general relation exhibited by Mr. Boole's fundamental theorem of development connecting any system of linear differential equations with a corresponding system of equations in finite differences, are merely different representations of a part of some more general method or process.

The principal difference in results, so far as concerns the solution in series of linear differential equations, appears to be, that in this paper the law of relation of the
coefficients of each series is distinct, and substantially the same in form for all; and that it is not necessary to have recourse to the method of parameters in the cases of equal roots, or in the case of there being a term $G$ on the right-hand side of the equation. In the case of imaginary roots, the laws of the series are in the first instance distinct from each other, and afterwards combined in couples.

19. The investigations contained in the latter part of this paper reduce the problem of the integration in finite terms of the general linear differential equation with rational coefficients, to the finding of an algebraical expression representing the development

$$A_0 + A_1 x + A_2 x^2 + \ldots + A_m x^m + \ldots,$$

where $A_0 = 1$, and the law of relation is

$$f_0 mA_m + f_1 mA_{m-1} + f_2 mA_{m-2} + \ldots + f_r mA_{m-r} = 0;$$

the functions $f_0, f_1, \ldots, f_r$ being known specific functions. The series to be summed closely resembles a recurring series; it differs from it in this particular, that the law of relation, instead of being constant, has a uniform and simple variation as it progresses along the series. If the rational coefficients should be themselves infinite series, the process still applies, the only difference being that each term would be formed from all the preceding terms, instead of being formed from the $r$ immediately preceding terms, or from all the preceding terms when the number of them is less than $r$.

In those cases in which the equation is soluble in finite terms by known methods, we are enabled to assign the algebraical expression for the series; a result which may be used for the discovery of generating functions.

Thus, taking the general equation of the first order,

$$\left(a_1 + b_1 x + c_1 x^2 + \ldots + l_1 x^n\right) \frac{du}{dx} + \left(a_0 + b_0 x + c_0 x^2 + \ldots + k_0 x^{n-1}\right) u = 0,$$

we see that

$$e^{-\int \frac{a_0 + b_0 x + \ldots + k_0 x^{n-1}}{a_1 + b_1 x + \ldots + l_1 x^n} dx} = A_0 + A_1 x + A_2 x^2 + \ldots + A_m x^m + \ldots,$$

where $A_0 = 1$, and the general law of the series is

$$a_1 mA_m + (b_1(m-1) + a_0) A_{m-1} + (c_1(m-2) + b_0) A_{m-2} + \ldots (l_1(m-n) + k_0) A_{m-n} = 0.$$

If we take the general equation of the second order,

$$\left(a_2 + b_2 x + \ldots + l_2 x^n\right) \frac{d^2u}{dx^2} + \left(a_1 + b_1 x + \ldots + k_1 x^{n-1}\right) \frac{du}{dx} + \left(a_0 + b_0 x + \ldots + h_0 x^{n-2}\right) u = 0,$$

the law of the series will be

$$a_2 m(m-1) A_m + (b_2(m-1)(m-2) + a_1(m-1)) A_{m-1} + (c_2(m-2)(m-3) + b_1(m-2) + a_0) A_{m-2} + \ldots + (l_2(m-n)(m-n-1) + k_1(m-n) + h_0) A_{m-n} = 0;$$

and generally, in equations of the $n$th order, the law of relation involves the $n$th power of $m$, the number of the term sought for. Thus the determination of soluble
cases of linear equations resolves itself into the inquiry, in what cases can a series whose scale is of the $n$th order be resolved into a number of series whose scale is of the first order.

20. I have not thought it necessary here to extend these solutions in series to linear partial differential equations. The process by which the extension can be made is well known, and has no peculiar relation to the methods here developed.