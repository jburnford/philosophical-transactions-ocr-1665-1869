# A Supplementary Memoir on the Theory of Matrices

**Author(s):** A. Cayley  
**Year:** 1866  
**Journal:** Philosophical Transactions of the Royal Society of London  
**Volume:** 156  
**Pages:** 12 pages  
**Identifier:** jstor-108937  
**JSTOR URL:** <https://www.jstor.org/stable/108937>  

---

II. A Supplementary Memoir on the Theory of Matrices.

By A. Cayley, Esq., F.R.S.

Received October 24,—Read December 7, 1865.

M. Hermite, in a paper "Sur la théorie de la transformation des fonctions Abéliennes," Comptes Rendus, t. xl. (1855), pp. 249, &c., establishes incidentally the properties of the matrix for the automorphic linear transformation of the bipartite quadric function $xw' + yz' - zy' - wx'$, or transformation of this function into one of the like form, $XW' + YZ' - ZY' - WX'$. These properties are (as will be shown) deducible from a general formula in my "Memoir on the Automorphic Linear Transformation of a Bipartite Quadric Function," Phil. Trans. vol. cxlviii. (1858), pp. 39–46; but the particular case in question is an extremely interesting one, the theory whereof is worthy of an independent investigation. For convenience the number of variables is taken to be four; but it will be at once seen that as well the demonstrations as the results are in fact applicable to any even number whatever of variables.

Article Nos. 1 & 2.—Notation and Remarks.

1. I use throughout the notation and formulæ contained in my "Memoir on the Theory of Matrices," Phil. Trans. vol. cxlviii. (1858), pp. 17–37, and in the above-mentioned memoir on the Automorphic Transformation. With respect to the composition of matrices, the rule of composition is as follows, viz., any line of the compound matrix is obtained by combining the corresponding line of the first or further component matrix with the several columns of the second or nearer component matrix; it is very convenient to indicate this by the algorithm,

$$\begin{pmatrix}
(a, b, c) & (\alpha, \beta, \gamma) \\
(a', b', c') & (\alpha', \beta', \gamma') \\
(a'', b'', c'') & (\alpha'', \beta'', \gamma'')
\end{pmatrix} = \begin{pmatrix}
(a, b, c) & (\alpha, \beta, \gamma) \\
(a', b', c') & (\alpha', \beta', \gamma') \\
(a'', b'', c'') & (\alpha'', \beta'', \gamma'')
\end{pmatrix}$$

which exhibits very clearly the terms which are to be combined together; thus in the upper left-hand corner we have $(a, b, c)(\alpha, \beta, \gamma)$, and so for the other places in the compound matrix.

2. It is not in the Memoir on Matrices explicitly remarked, but it is easy to see that sums of matrices, all the matrices being of the same order, may be multiplied together by the ordinary rule; thus

$$(A+B)(C+D)=AC+AD+BC+BD:$$

this remark will be useful in the sequel.

MDCCCLXVI.
Article Nos. 3 to 13.—First Investigation.

3. We have to consider the formulæ for the automorphic linear transformation of the function \(xw' + yz' - zy' - wx'\), that is, of the function

\[
\begin{vmatrix}
\cdot & \cdot & \cdot & -1 \\
\cdot & \cdot & -1 & \cdot \\
\cdot & 1 & \cdot & \cdot \\
1 & \cdot & \cdot & \cdot
\end{vmatrix}
(x, y, z, w; x', y', z', w')
\]

\(= (\Omega (x, y, z, w; x', y', z', w'),\)

viz., if the variables are transformed by the formulæ

\[
(x, y, z, w) = (\Pi (X, Y, Z, W),
(x', y', z', w') = (\Pi (X', Y', Z', W'),
\]

then the matrix \((\Pi)\) is such that we have identically

\[
(\Omega (x, y, z, w; x', y', z', w') = (\Omega (X, Y, Z, W; X', Y', Z', W');
\]

the expression for \((\Pi)\) is given in my memoir above referred to; viz., observing that the matrix \((\Omega)\) is skew symmetrical, then (No. 13) we have

\[
\Pi = \Omega^{-1}(\Omega - \Upsilon)(\Omega + \Upsilon)^{-1}\Omega,
\]

where \(\Upsilon\) is an arbitrary symmetrical matrix.

4. I propose to compare with the matrix \(\Pi\) the inverse matrix \(\Pi^{-1}\). Recollecting that in the theory of matrices \((ABCD)^{-1} = D^{-1}C^{-1}B^{-1}A^{-1}\), we have

\[
\Pi^{-1} = \Omega^{-1}(\Omega + \Upsilon)(\Omega - \Upsilon)^{-1}\Omega;
\]

and it is to be shown that \(\Pi\) and \(\Pi^{-1}\) are composed of terms which (except as to their signs) are the same in each, so that either of these matrices is derivable from the other by a peculiar form of transposition. It is to be borne in mind throughout that \(\Upsilon\) is symmetrical, \(\Omega\) skew symmetrical.

5. I write for greater convenience

\[
-\Pi = \Omega^{-1}(\Upsilon - \Omega)(\Upsilon + \Omega)^{-1}\Omega,
\]

\[
-\Pi^{-1} = \Omega^{-1}(\Upsilon + \Omega)(\Upsilon - \Omega)^{-1}\Omega,
\]

and I compare in the first instance the matrices \((\Upsilon - \Omega)(\Upsilon + \Omega)^{-1}\) and \((\Upsilon + \Omega)(\Upsilon - \Omega)^{-1}\).

6. Any matrix whatever, and therefore the matrix \((\Upsilon + \Omega)^{-1}\), may be exhibited as the sum of a symmetrical matrix and a skew symmetrical matrix; that is, we may write

\[
(\Upsilon + \Omega)^{-1} = \Upsilon' + \Omega',
\]

where \(\Upsilon'\) is symmetrical, \(\Omega'\) is skew symmetrical. We have then

\[
(\Upsilon + \Omega)(\Upsilon + \Omega)^{-1} = (\Upsilon + \Omega)(\Upsilon' + \Omega') = 1,
\]
where, here and in what follows, 1 denotes the matrix unity. Moreover

\[ \Upsilon - \Omega = \text{tr.}(\Upsilon + \Omega), \]

and thence

\[ (\Upsilon - \Omega)^{-1} = (\text{tr.}(\Upsilon + \Omega))^{-1} = \text{tr.}(\Upsilon' + \Omega') = \Upsilon' - \Omega'; \]

that is,

\[ (\Upsilon - \Omega)^{-1} = \Upsilon' - \Omega'; \]

and thence also

\[ (\Upsilon - \Omega)(\Upsilon - \Omega)^{-1} = (\Upsilon - \Omega)(\Upsilon' - \Omega') = 1. \]

We have therefore

\[ (\Upsilon - \Omega)(\Upsilon + \Omega)^{-1} = (\Upsilon + \Omega - 2\Omega)(\Upsilon' + \Omega') = 1 - 2\Omega(\Upsilon' + \Omega'), \]

\[ (\Upsilon + \Omega)(\Upsilon - \Omega)^{-1} = (\Upsilon - \Omega + 2\Omega)(\Upsilon' - \Omega') = 1 + 2\Omega(\Upsilon' - \Omega'). \]

7. Suppose for a moment that

\[ \Upsilon' + \Omega' = \begin{pmatrix} a, b, c, d \\ e, f, g, h \\ i, j, k, l \\ m, n, o, p \end{pmatrix} \]

and therefore

\[ \Upsilon' - \Omega' = \begin{pmatrix} a, e, i, m \\ b, f, j, n \\ c, g, k, o \\ d, h, l, p \end{pmatrix} \]

8. We have

\[ -\Omega(\Upsilon' + \Omega') = \begin{pmatrix} . & . & . & 1 \\ . & . & 1 & . \\ . & -1 & . & . \\ -1 & . & . & . \end{pmatrix} \begin{pmatrix} a, b, c, d \\ e, f, g, h \\ i, j, k, l \\ m, n, o, p \end{pmatrix} \]

\[ = \begin{pmatrix} . & . & . & 1 \\ . & . & 1 & . \\ . & -1 & . & . \\ -1 & . & . & . \end{pmatrix} \begin{pmatrix} a, e, i, m \\ b, f, j, n \\ c, g, k, o \\ d, h, l, p \end{pmatrix} \]

\[ = \begin{pmatrix} m, n, o, p \\ i, j, k, l \\ -e, -f, -g, -h \\ -a, -b, -c, -d \end{pmatrix}. \]
9. And similarly,

\[ \Omega(\Upsilon' - \Omega') = \begin{pmatrix} \cdot & \cdot & \cdot & -1 \\ \cdot & \cdot & -1 & \cdot \\ \cdot & 1 & \cdot & \cdot \\ 1 & \cdot & \cdot & \cdot \end{pmatrix} \begin{pmatrix} a, e, i, m \\ b, f, j, n \\ c, g, k, o \\ d, h, l, p \end{pmatrix} \]

\[ = \begin{pmatrix} \cdot & \cdot & \cdot & -1 \\ \cdot & \cdot & -1 & \cdot \\ \cdot & 1 & \cdot & \cdot \\ 1 & \cdot & \cdot & \cdot \end{pmatrix} \begin{pmatrix} (a, b, c, d), (e, f, g, h), (i, j, k, l), (m, n, o, p) \end{pmatrix} \]

\[ = (-d, -h, -l, -p), \begin{pmatrix} -c, -g, -k, -o \\ b, f, j, n \\ a, e, i, m \end{pmatrix} \]

10. Hence also

\[ (\Upsilon - \Omega)(\Upsilon + \Omega)^{-1} = \begin{pmatrix} 1+2m, 2n, 2o, 2p \\ 2i, 1+2j, 2k, 2l \\ -2e, -2f, 1-2g, -2h \\ -2a, -2b, -2c, 1-2d \end{pmatrix}, \]

and

\[ (\Upsilon + \Omega)(\Upsilon - \Omega)^{-1} = \begin{pmatrix} 1-2d, -2h, -2l, -2p \\ -2c, 1-2g, -2k, -2o \\ 2b, 2f, 1+2j, 2n \\ 2a, 2e, 2i, 1+2m \end{pmatrix}, \]

so that these matrices are composed of terms which, except as to the signs, are the same in each.

11. Now in general if

\[ \Theta = \begin{pmatrix} \alpha, \beta, \gamma, \delta \\ \alpha', \beta', \gamma', \delta' \\ \alpha'', \beta'', \gamma'', \delta'' \\ \alpha''', \beta''', \gamma''', \delta''' \end{pmatrix}, \]

then it is easy to see that

\[ \Omega^{-1}\Theta\Omega = \begin{pmatrix} \delta'', \gamma'', -\beta'', -\alpha'' \\ \delta'', \gamma'', -\beta'', -\alpha'' \\ -\delta', -\gamma', \beta', \alpha' \\ -\delta, -\gamma, \beta, \alpha \end{pmatrix}. \]
and hence, from the foregoing values of \((\Upsilon - \Omega)(\Upsilon + \Omega)^{-1}\) and \((\Upsilon + \Omega)(\Omega - \Upsilon)^{-1}\), we find

\[
\Pi = -\Omega^{-1}(\Upsilon - \Omega)(\Upsilon + \Omega)^{-1}\Omega = \begin{pmatrix}
-1+2d & 2c & -2b & -2a \\
2h & -1+2g & -2f & -2e \\
2l & 2k & -1-2j & -2i \\
2p & 2o & -2n & -1-2m
\end{pmatrix}
\]

and

\[
\Pi^{-1} = -\Omega^{-1}(\Upsilon + \Omega)(\Upsilon - \Omega)^{-1}\Omega = \begin{pmatrix}
-1-2m & -2i & 2e & 2a \\
-2n & -1-2j & 2f & 2b \\
-2o & -2k & -1+2g & 2c \\
-2p & -2l & +2h & -1+2d
\end{pmatrix}
\]

which shows that the matrix \(\Pi\) for the automorphic transformation of the function \(xw' + yz' - zy' - wx'\) is such that writing

\[
\Pi = (A, B, C, D) \quad \text{we have} \quad \Pi^{-1} = (P, L, -H, -D)
\]

\[
E, F, G, H \quad O, K, -G, -C
\]

\[
I, J, K, L \quad -N, -J, F, B
\]

\[
M, N, O, P \quad -M, -I, E, A
\]

which is the theorem in question.

12. I remark in reference to the foregoing proof that writing

\[
\Upsilon = \begin{pmatrix}
a, h, g, l \\
h, b, f, m \\
g, f, c, n \\
l, m, n, p
\end{pmatrix}
\]

then the actual value of

\[
(\Upsilon + \Omega)^{-1} = \begin{pmatrix}
a, h, g, l-1 \\
h, b, f-1, m \\
g, f+1, c, n \\
l+1, m, n, d
\end{pmatrix}^{-1}
\]

is

\[
= \frac{1}{\Delta} \begin{pmatrix}
A+d, H+n+v, G-m-\mu, L-l+\varepsilon \\
H+n-v, B+c, F-f+\lambda, M-g+\sigma \\
G-m+\mu, F-f-\lambda, C+b, N+h+\tau \\
L-l-\varepsilon, M-g-\sigma, N+h-\tau, D+a
\end{pmatrix}
\]
where

\[
\begin{pmatrix}
A & H & G & L \\
H & B & F & M \\
G & F & C & N \\
L & M & N & D
\end{pmatrix}
\]

is the matrix formed with the first minors of

\[
\begin{pmatrix}
a & h & g & l \\
h & b & f & m \\
g & f & c & n \\
l & m & n & d
\end{pmatrix};
\]

moreover

\[
\lambda = ad - l^2 + nh - mg + 1, \quad \varepsilon = bc - f^2 + nh - mg + 1,
\]
\[
\mu = bn - mf + dh - ml, \quad \sigma = fg - ch + gl - na,
\]
\[
\nu = dg - nl + nf - cm, \quad \tau = hf - bg + ma - lh,
\]

and \( \Delta \) is the determinant

\[
\begin{pmatrix}
a & h & g & l+1 \\
h & b & f+1 & m \\
g & f-1 & c & n \\
l-1 & m & n & d
\end{pmatrix}
\]

viz., this is

\[
= \begin{vmatrix}
a & h & g & l \\
h & b & f & m \\
g & f & c & n \\
l & m & n & d
\end{vmatrix} + ad - l^2 + bc - f^2 + 2(nh - mg) + 1.
\]

13. The expression for \((Y - \Omega)^{-1}\) is obtained from that of \((Y + \Omega)^{-1}\) by merely transposing the terms of the matrix, or, what is the same thing, by changing the signs of \( \lambda, \mu, \nu, \varepsilon, \sigma, \tau \). And it would be easy by means of these developed values to verify the foregoing comparison of \((Y - \Omega)(Y + \Omega)^{-1}\) and \((Y + \Omega)(Y - \Omega)^{-1}\).

Article Nos. 14 to 22.—Second Investigation.

14. I consider from a different point of view the theory of a matrix

\[
\Pi = \begin{pmatrix}
a & b & c & d \\
e & f & g & h \\
i & j & k & l \\
m & n & o & p
\end{pmatrix}
\]

such that \( \Pi^{-1} = \begin{pmatrix}
p & l & -h & -d \\
o & k & -g & -c \\
-n & -j & f & b \\
-m & -i & e & a
\end{pmatrix},
\]

or, as we may call it, a Hermitian matrix.
15. **Lemma.** The determinant

\[ \nabla = \begin{vmatrix} a, & b, & c, & d \\ e, & f, & g, & h \\ i, & j, & k, & l \\ m, & n, & o, & p \end{vmatrix} \]

may be expressed, and that in two different ways, as a Pfaffian.

16. In fact multiplying the determinant into itself thus,

\[ \nabla^2 = \begin{vmatrix} a, & b, & c, & d \\ e, & f, & g, & h \\ i, & j, & k, & l \\ m, & n, & o, & p \end{vmatrix} \text{ tr. } \begin{vmatrix} d, & c, & -b, & -a \\ h, & g, & -f, & -e \\ l, & k, & -j, & -i \\ p, & o, & -n, & -m \end{vmatrix} \]

we find

\[ \nabla^2 = (a, b, c, d) \begin{vmatrix} s_{11}, & s_{12}, & s_{13}, & s_{14} \\ s_{21}, & s_{22}, & s_{23}, & s_{24} \\ s_{31}, & s_{32}, & s_{33}, & s_{34} \\ s_{41}, & s_{42}, & s_{43}, & s_{44} \end{vmatrix} \]

viz. we have \( s_{11} = (a, b, c, d)(d, c, -b, -a), s_{12} = (a, b, c, d)(h, g, -f, -e), \&c. \): we see at once that \( s_{11} = 0, s_{12} + s_{21} = 0, \&c., \) viz. the determinant in \( s \) is a skew determinant, that is, the square of a Pfaffian. We have therefore

\[ \nabla^2 = (s_{12}s_{24} + s_{13}s_{42} + s_{14}s_{23})^2, \]

or extracting the square root of each side, and determining the sign by a comparison of any single term, we have

\[ \nabla = s_{12}s_{34} + s_{13}s_{42} + s_{14}s_{23}, \]

which is one of the required forms of \( \nabla \).

17. And in the same manner

\[ \nabla^2 = \text{ tr. } \begin{vmatrix} a, & b, & c, & d \\ e, & f, & g, & h \\ i, & j, & k, & l \\ m, & n, & o, & p \end{vmatrix} \cdot \begin{vmatrix} m, & n, & o, & p \\ i, & j, & k, & l \\ -e, & -f, & -g, & -h \\ -a, & -b, & -c, & -d \end{vmatrix} \]

which is equal to the determinant

\[ \begin{vmatrix} t_{11}, & t_{12}, & t_{13}, & t_{14} \\ t_{21}, & t_{22}, & t_{23}, & t_{24} \\ t_{31}, & t_{32}, & t_{33}, & t_{34} \\ t_{41}, & t_{42}, & t_{43}, & t_{44} \end{vmatrix} = (a, e, i, m) \begin{vmatrix} s_{11}, & s_{12}, & s_{13}, & s_{14} \\ s_{21}, & s_{22}, & s_{23}, & s_{24} \\ s_{31}, & s_{32}, & s_{33}, & s_{34} \\ s_{41}, & s_{42}, & s_{43}, & s_{44} \end{vmatrix} \]
viz. \( t_{11} = (a, e, i, m) \times (m, i, -e, -a), \) &c.; this is likewise a skew determinant, and we have

\[
\nabla^2 = (t_{12} t_{34} + t_{13} t_{24} + t_{14} t_{23})^2,
\]

or extracting the square root of each side, and determining the sign by the comparison of any single term, we have

\[
\nabla = t_{12} t_{34} + t_{13} t_{24} + t_{14} t_{23},
\]

which is the other of the required forms of \( \nabla \).

18. Consider now the matrix

\[
\begin{pmatrix}
a, b, c, d \\
e, f, g, h \\
i, j, k, l \\
m, n, o, p
\end{pmatrix}
\]

which is such that

\[
\begin{pmatrix}
a, b, c, d \\
e, f, g, h \\
i, j, k, l \\
m, n, o, p
\end{pmatrix}^{-1} =
\begin{pmatrix}
p, l, -h, -d \\
o, k, -g, -c \\
-n, -j, f, b \\
-m, -i, e, a
\end{pmatrix}
\]

this gives

\[
\begin{pmatrix}
1, 0, 0, 0 \\
0, 1, 0, 0 \\
0, 0, 1, 0 \\
0, 0, 0, 1
\end{pmatrix} =
\begin{pmatrix}
a, b, c, d \\
e, f, g, h \\
i, j, k, l \\
m, n, o, p
\end{pmatrix}
\begin{pmatrix}
p, l, -h, -d \\
o, k, -g, -c \\
-n, -j, f, b \\
-m, -i, e, a
\end{pmatrix}
\]

\[
= (a, b, c, d)
\begin{pmatrix}
p, o, -n, -m \\
l, k, -j, -i \\
-h, -g, f, e \\
-d, -c, b, a
\end{pmatrix}
\]

which is in fact

\[
\begin{pmatrix}
1, 0, 0, 0 \\
0, 1, 0, 0 \\
0, 0, 1, 0 \\
0, 0, 0, 1
\end{pmatrix} =
\begin{pmatrix}
s_{14}, s_{13}, -s_{12}, -s_{11} \\
s_{24}, s_{23}, -s_{22}, -s_{21} \\
s_{34}, s_{33}, -s_{32}, -s_{31} \\
s_{44}, s_{43}, -s_{42}, -s_{41}
\end{pmatrix},
\]

and the two matrices will be equal, term by term, if only

\[
1 = s_{14} = s_{23},
\]

\[
0 = s_{13} = s_{12} = s_{24} = s_{34},
\]

that is, if six conditions are satisfied.
19. But we have also (a matrix and its reciprocal being convertible)

\[
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
=
\begin{pmatrix}
p & l & -h & -d \\
o & k & -g & -c \\
-n & -j & f & b \\
-m & -i & e & a
\end{pmatrix}
\]

which is in fact

\[
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
=
\begin{pmatrix}
t_{14} & t_{24} & t_{34} & t_{44} \\
t_{13} & t_{23} & t_{33} & t_{43} \\
-t_{12} & -t_{22} & -t_{32} & -t_{42} \\
-t_{11} & -t_{21} & -t_{31} & -t_{41}
\end{pmatrix}
\]

and we obtain for the equality of the two matrices the six conditions

\[1 = t_{14} = t_{23}, \quad 0 = t_{13} = t_{12} = t_{24} = t_{34},\]

equivalent to the former set of six conditions.

20. We obtain from either set of conditions, for the determinant the value

\[
\nabla = \begin{vmatrix}
a & b & c & d \\
e & f & g & h \\
i & j & k & l \\
m & n & o & p
\end{vmatrix} = x^2.
\]

21. Write

\[
(x, y, z, w) = \begin{pmatrix}
a & b & c & d \\
e & f & g & h \\
i & j & k & l \\
m & n & o & p
\end{pmatrix} (X, Y, Z, W); \quad (x', y', z', w') = \begin{pmatrix}
a & b & c & d \\
e & f & g & h \\
i & j & k & l \\
m & n & o & p
\end{pmatrix} (X', Y', Z', W'),
\]

then substituting for \((x, y, z, w)(x', y', z', w')\) their values, we find

\[xw' + yz' - zy' - wx' = -\begin{vmatrix}
t_{11} & t_{12} & t_{13} & t_{14} \\
t_{21} & t_{22} & t_{23} & t_{24} \\
t_{31} & t_{32} & t_{33} & t_{34} \\
t_{41} & t_{42} & t_{43} & t_{44}
\end{vmatrix} (X, Y, Z, W)(X', Y', Z', W')\]
\[
= \begin{pmatrix}
\cdot & \cdot & \cdot & -1 \\
\cdot & \cdot & -1 & \cdot \\
\cdot & 1 & \cdot & \cdot \\
1 & \cdot & \cdot & \cdot
\end{pmatrix} (X, Y, Z, W)(X', Y', Z', W')
\]

\[= XW' + YZ' - ZY' - WX';\]

and similarly writing

\[(X, Y, Z, W) = \begin{pmatrix}
p & l & -h & -d \\
o & k & -g & -c \\
-n & -j & f & b \\
-m & -i & e & a
\end{pmatrix} (x, y, z, w),\]
\[(X', Y', Z', W') = \begin{pmatrix}
p & l & -h & -d \\
o & k & -g & -c \\
-n & -j & f & b \\
-m & -i & e & a
\end{pmatrix} (x', y', z', w'),\]

we obtain with the \(s\) coefficients the equivalent result,

\[XW' + YZ' - ZY' - WX' = xw' + yz' - zy' - wx'.\]

We thus see conversely that the Hermitian matrix is in fact the matrix for the automorphic transformation of the function \(xw' + yz' - zy' - wx'\).

22. Considering any two or more matrices for the automorphic transformation of such a function, the matrix compounded of these is a matrix for the automorphic transformation of the function—or, theorem, the matrix compounded of two or more Hermitian matrices is itself Hermitian.

Article No. 23.—Theorem on a Form of Matrices.

23. I take the opportunity of mentioning a theorem relating to the matrices which present themselves in the arithmetical theory of the composition of quadratic forms.

Writing

\[(X) = \begin{pmatrix}
\cdot & \alpha & a & b + \beta \\
-\alpha & \cdot & b - \beta & c \\
-\alpha & -(b - \beta) & \cdot & \gamma \\
-(b + \beta) & -c & -\gamma & \cdot
\end{pmatrix}\]

and \((X)^{-1} = \frac{1}{D - \Delta} \begin{pmatrix}
\cdot & \gamma & -c & b - \beta \\
-\gamma & \cdot & b + \beta & -a \\
c & -(b + \beta) & \cdot & \alpha \\
-(b - \beta) & a & -\alpha & \cdot
\end{pmatrix},\)

where \(D = ac - b^2, \Delta = a\gamma - b^2\); and similarly,

\[(X') = \begin{pmatrix}
\cdot & \alpha' & a' & b' + \beta' \\
-\alpha' & \cdot & b' - \beta' & c' \\
-\alpha' & -(b' - \beta') & \cdot & \gamma' \\
-(b' + \beta') & -c' & -\gamma' & \cdot
\end{pmatrix}\]

and \((X')^{-1} = \frac{1}{D' - \Delta'} \begin{pmatrix}
\cdot & \gamma' & -c' & b' - \beta' \\
-\gamma' & \cdot & b' + \beta' & -a' \\
c' & -(b' + \beta') & \cdot & \alpha' \\
-(b' - \beta') & a' & -\alpha' & \cdot
\end{pmatrix},\)

where \(D' = a'c' - b'^2, \Delta' = a'\gamma' - b'^2\); then

\[(X)(X') + (D - \Delta)(D' - \Delta')(X'^{-1})(X^{-1}),\]
or, what is the same thing,

$$(X \chi X') + (D - \Delta)(D' - \Delta')((X \chi X')^{-1}$$

is to a factor près equal to the matrix unity; viz. writing

$$\Lambda = aa + 2b\beta + c\gamma + a'\alpha' + 2b'\beta' + c'\gamma',$$

the foregoing expression is

$$= \Lambda \begin{pmatrix} 1 & . & . & . \\ . & 1 & . & . \\ . & . & 1 & . \\ . & . & . & 1 \end{pmatrix}.$$  

The theorem is verified without difficulty by merely forming the expressions of the compound matrices $(X \chi X')$ and $(X'^{-1} \chi X^{-1})$.